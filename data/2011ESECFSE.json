conference({
	"DataRevision": 21, 

	"Event": "ESECFSE2011",
	"Name": "ESEC-FSE 2011",
	"HawaiiGroup": "ESECFSE2011",
	"VenueInfo" : {
		"Name": "University Congress Centre, University of Szeged, Szeged, Hungary",
		"GPS": {"Latitude": 46.247003, "Longitude": 20.142209}   
	}, 
	"InfoPage": {
		"xaml": "foo",
		"Elements": [
			{
				"XamlName": "HotelMap",
				"Type": "Map",
				"GPS": {"Latitude": 46.247003, "Longitude": 20.142209}, 
				"MapLabel": "University Congress Centre"
			},
			{
				"XamlName": "LicenseButton",
				"Type": "WebLink",
				"URL": "http://research.microsoft.com/en-US/projects/confapp/terms.aspx"
			},
			{
				"XamlName": "PrivacyButton",
				"Type": "WebLink",
				"URL": "http://privacy.microsoft.com/en-us/default.msp"
			}
		]
	},
	"SessionPriorities" : [
                    "Plenary", "Keynote",  "Other"],	"Items": [
{
  "Title": "An Architecture-centric Approach for Goal-driven Requirements Elicitation",
  "Type": "Paper",
  "Key": "fse03d",
  "Authors": ["Zoya Durdik"],
  "Affiliations": ["Research Center for Information Technology (FZI)"],
  "Abstract": "Software system development typically starts from a requirement specification followed by stepwise refinement of available requirements while transferring them into the system architecture. However, the granularity and the amount of requirements to be elicited for a successful architectural design are not well understood. This paper proposes a process concept to support system development with the help of an architecture-centric approach for goal-driven requirements elicitation. The process focuses on multiple quality dimensions, such as performance, reliability and scalability, and at the same time shall reduce costs and risks through early decision evaluation. The main contribution of this paper is a novel process where not only requirements can drive architectural design, but also architectural design can selectively drive requirement elicitation with the help of hypotheses connected to the selected architectural solutions. The paper concludes with a discussion on its possible empirical validation."
},
{
  "Title": "Reputation-based Self-management of Software Process Artifact Quality in Consortium Research Projects",
  "Type": "Paper",
  "Key": "fse06d",
  "Authors": ["Christian Prause"],
  "Affiliations": ["Fraunhofer FIT"],
  "Abstract": "This paper proposes a PhD research that deals with improving internal documentation in software projects. Software developers often do not like to create documentation because it has few value to the individual himself. Yet the purpose of internal documentation is to help others understand the software and the problem it addresses. Documentation increases development speed, reduces software maintenance costs, helps to keep development on track, and mitigates the negative effects of distance in distributed settings. This research aims to increase the individuals' motivation to write documentation by means of reputation. The CollabReview prototype is a web-based reputation system that analyzes artifacts of internal documentation to derive personal reputation scores. Developers making many good contributions will achieve higher reputation scores. These scores can then be employed to softly influence developer behavior, e.g. by incentivizing them to contribute to documentation with social rewards. No strict rule enforcement is necessary."
},
{
  "Title": "Mining Development Repositories To Study the Impact of Collaboration on Software Systems",
  "Type": "Paper",
  "Key": "fse08d",
  "Authors": ["Nicolas Bettenburg"],
  "Affiliations": ["Queen's University, School of Computing"],
  "Abstract": "Software development is a largely collaborative effort, of which the actual encoding of program logic in source code is a relatively small part. Yet, little is known about the impact of collaboration between stakeholders on software quality. We hypothesize that the collaboration between stakeholders during software development has a non-negligible impact on the software system. Information about collaborative activities can be recovered from traces of their communication, which are recorded in the repositories used for the development of the software system. This thesis contributes the following 1) to make this information accessible for practitioners and researchers, we present approaches to distill communication information from development repositories, and empirically validate our proposed extractors. 2) By linking back the extracted communication data to the parts of the software system under discussion, we are able to empirically study the impact of communication, as a proxy to collaboration between stakeholders, on a software system. Through case studies on a broad spectrum of open-source software projects, we demonstrate the important role of social interactions between stakeholders with respect to the evolution of a software system."
},
{
  "Title": "Experimental Specification Mining for Enterprise Applications",
  "Type": "Paper",
  "Key": "fse01d",
  "Authors": ["Matthias Schur"],
  "Affiliations": ["SAP Research"],
  "Abstract": "Specification mining infers abstractions over a set of program execution traces. Whereas inductive approaches to specification mining rely on a given set of execution traces, experimental approaches systematically generate and execute test cases to infer rich models including uncommon and exceptional behavior. State-of-the-art experimental mining approaches infer low-level models representing the behavior of single classes. This paper proposes an approach for inferring models of built-in processes in enterprise systems based on systematic scenario test generation. The paper motivates the approach, sketches the relevant concepts and challenges, and discusses related work."
},
{
  "Title": "Search Based Hierarchy Generation for Reverse Engineered State Machines",
  "Type": "Paper",
  "Key": "fse11d",
  "Authors": ["Mathew Hall"],
  "Affiliations": ["University of Sheffield"],
  "Abstract": "Abstraction is a valuable tool that can play an important role in reducing the cost of maintenance of software systems. Despite the cost reduction abstract documentation can provide, the cost of generating documentation that offers an implementation-independent overview of the system often outweighs it. This has been the motivating force for tools and techniques that reduce the cost of documentation generation, including this work. State machines offer an ideal level of abstraction and techniques to infer them from machines are already mature. Despite this, the abstraction state machines provide is restricted as they become unmanageable when they are of any significant size. As a result, inference tools are only ideal for those who are already familiar with the system. This work focuses on making state machines useful for larger systems. In order to do so the complexity of a machine needs to be reduced; this is realised by introducing a hierarchy to the machine, making them closer to Harel's Statechart formalism (without concurrency)."
},
{
  "Title": "Automatic Test Suite Evolution",
  "Type": "Paper",
  "Key": "fse04d",
  "Authors": ["Mehdi Mirzaaghaei"],
  "Affiliations": ["University of Lugano"],
  "Abstract": "Software evolves continuously, and developers need to retest it frequently. To save time and effort, developers often reuse existing test cases to verify the functionality of software systems after changes, but they often need to adapt or augment the test cases to match the new characteristics of the software systems. Adapting test cases is tedious and expensive. Current automated techniques often generate invalid and incomplete test cases, and require manual inspection and correction of the generated test cases. My research aims to introduce new automated approaches for evolving and generating test cases, to keep them aligned with the corresponding software evolution. The new approach is based on the observation that software developers follow common patterns to identify changes and adapt test cases. I experimentally identified patterns that developers use in presence of specific changes, and I am working on an automated approach that generalizes these patterns into a set of test adaptation patterns that can automatically evolve existing test cases and generate new ones. My preliminary evaluation shows the applicability and effectiveness of the approach."
},
{
  "Title": "Automatic Structural Testing with Abstraction Refinement and Coarsening",
  "Type": "Paper",
  "Key": "fse05d",
  "Authors": ["Mauro Baluda"],
  "Affiliations": ["University of Lugano"],
  "Abstract": "White box testing, also referred to as structural testing, can be used to assess the validity of test suites with respect to the implementation. The applicability of white box testing and structural coverage is limited by the difficulty and the cost of inspecting the uncovered code elements to either generate test cases that cover elements not yet executed or to prove the infeasibility of the elements not yet covered. My research targets the problem of increasing code coverage by automatically generating test cases that augment the coverage of the code or proving the infeasibility of uncovered elements, and thus eliminating them from the coverage measure to obtain more realistic values. Although the problem is undecidable in general, the results achieved so far during my PhD indicate that it is possible to extend the test suites and identify many infeasible elements by suitably combining static and dynamic analysis techniques, and that it is possible to manage the combinatorial explosion of execution models by identifying and remove elements of the execution models when not needed anymore."
},
{
  "Title": "Understanding Failures Through Facts",
  "Type": "Paper",
  "Key": "fse10d",
  "Authors": ["Jeremias RÃ¶Ãler"],
  "Affiliations": ["Saarland University"],
  "Abstract": "\u001CWhy does my program crash This ever recurring question of software debugging drives the developer during the analysis of the failure. Complex defects are impossible to automatically identify; this can only be left to human judgment. But what we can do is empower the developer to make an informed decision, by helping her understand the failure. To fully comprehend a failure, one may need to consider many different aspects such as the range of the input parameters and the program\u0019s structure and runtime behavior. I propose an approach that gathers a variety of such facts from a given failing execution. To examine the correlation of those facts to the failure, it produces additional executions that differ in as few facts as possible. Then the approach creates generalizations and abstractions over the correlating facts. These explain different aspects of the failure and thus help the developer understand and eventually fix the underlying defect."
},
{
  "Title": "QoS Verification and Model Tuning @ Runtime",
  "Type": "Paper",
  "Key": "fse07d",
  "Authors": ["Antonio Filieri"],
  "Affiliations": ["Politecnico di Milano"]
},
{
  "Title": "A Software Lifecycle Process for Context-Aware Adaptive Systems",
  "Type": "Paper",
  "Key": "fse09d",
  "Authors": ["Marco Mori"],
  "Affiliations": ["IMT Institute for Advanced Studies Lucca"],
  "Abstract": "It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated."
},
{
  "Title": "How to perform a reliable software engineering empirical study",
  "Type": "Talk",
  "Key": "Howtoperformareliablesoftwareengineeringempiricalstudy",
  "Authors": ["Prem Devanbu"]
},
{
  "Title": "How to write an excellent software engineering paper",
  "Type": "Talk",
  "Key": "Howtowriteanexcellentsoftwareengineeringpaper",
  "Authors": ["Laurie Williams"]
},
{
  "Title": "Hybrid Analysis for JavaScript Security Assessment",
  "Type": "Paper",
  "Key": "HybridAnalysisforJavaScriptSecurityAssessment",
  "Authors": ["Omer Tripp", "Omri Weisman"]
},
{
  "Title": "Productivity in IT services (Keynote)",
  "Type": "Paper",
  "Key": "ProductivityinITservicesKeynote",
  "Authors": ["Satish Chandra"]
},
{
  "Title": "Concolic Testing on Embedded Software - Case Studies on Mobile Platform Programs",
  "Type": "Paper",
  "Key": "ConcolicTestingonEmbeddedSoftwareCaseStudiesonMobilePlatformPrograms",
  "Authors": ["Yunho Kim", "Moonzoo Kim", "Yoonkyu Jang"]
},
{
  "Title": "Faster Fault Finding at Google using Multi Objective Regression Test Optimisation",
  "Type": "Paper",
  "Key": "FasterFaultFindingatGoogleusingMultiObjectiveRegressionTestOptimisation",
  "Authors": ["Shin Yoo", "Robert Nilsson", "Mark Harman"]
},
{
  "Title": "Managing Performance Testing With Release Certification and Data Correlation",
  "Type": "Paper",
  "Key": "ManagingPerformanceTestingWithReleaseCertificationandDataCorrelation",
  "Authors": ["Tuli Nivas", "Christoph Csallner"]
},
{
  "Title": "A True Story of Refactoring a Large Oracle PL/SQL Banking System",
  "Type": "Paper",
  "Key": "ATrueStoryofRefactoringaLargeOraclePLSQLBankingSystem",
  "Authors": ["Csaba Nagy", "Rudolf Ferenc", "Tibor Bakota"]
},
{
  "Title": "Automotive System Development Based on Collaborative Modeling Using Multiple ADLs",
  "Type": "Paper",
  "Key": "AutomotiveSystemDevelopmentBasedonCollaborativeModelingUsingMultipleADLs",
  "Authors": ["Shin'ichi Shiraishi", "Mutsumi Abe"]
},
{
  "Title": "Does Pair Programming Increase Developers Attention?",
  "Type": "Paper",
  "Key": "DoesPairProgrammingIncreaseDevelopersAttention",
  "Authors": ["Jelena Vlasenko", "Ilenia Fronza", "Alberto Sillitti", "Giancarlo Succi"]
},
{
  "Title": "Building Advanced Mechatronic Systems",
  "Type": "Keynote",
  "Key": "fse21ws",
  "Authors": ["Wilhelm SchÃ¤fer"],
  "Affiliations": ["University of Paderborn"],
  "Abstract": "Mechatronics is the engineering discipline concerned with the construction of systems incorporating mechanical, electronical and information technology components. Typical examples of mechatronic systems are automotive applications, e.g. advanced braking systems, fly steer-by-wire or active suspension techniques, but also DVD-players or washing machines. Mechatronic systems are characterised by a combination of basic mechanical devices with a processing unit monitoring and controlling it via a number of actuators and sensors. This leads to massive improvements in product performance and flexibility. The introduction of mechatronics as a tight integration of mechanical, electronical and information-driven units allowed for turning conventionally designed mechanical components into smart devices. Today we see the first steps in the emergence of the next generation of mechatronic systems. While intelligence in the behaviour has so far always been achieved by gathering information (and reacting to it) from the one single machine, the usage and retrieval of information in the future will be characterised by an exchange of information between different machines. This can for instance already be seen in the automotive and rail domain. Intelligent lighting systems combine information about their environment obtained from their own sensors with those collected by other cars. In the Paderborn rail system shuttles autonomously form convoys as to reduce air resistance and optimise energy consumption. In the talk we survey current state of the art in the development of mechatronic systems from a software engineering point of view. Based on identified weaknesses of existing approaches we present our own approach called Mechatronic UML. Mechatronic UML supports model-driven development of mechatronic systems addressing complex coordination between system components under hard real-time constraints and reconfiguration of control algorithms at runtime to adjust the system behaviour to changing system goals as well as target platform specific code generation. Modelling is based on a syntactically and semantically rigorously defined and partially refined subset of UML. It uses a slightly refined version of component diagrams, coordination patterns, and a refined version of state charts including the notion of time which are called Real time state charts. Verification of safety properties is based on a special kind of compositional model checking to make it scalable. Model checking exploits an underlying unifying semantics which is formally defined using graph transformation systems. The last part of the talk is devoted to pointing out future developments and research challenges which we believe characterise advanced mechatronic systems of the future."
},
{
  "Title": "ELI-ALPS -- The Ultrafast Challenges in Hungary",
  "Type": "Keynote",
  "Key": "fse23gz",
  "Authors": ["GÃ¡bor SzabÃ³"],
  "Affiliations": ["University of Szeged"],
  "Abstract": "The ELI -- Extreme Light Infrastructure -- or as it is commonly referred to. the SUPERLASER will be one of the large research facilities of the European Union. ELI will be built with a joint international effort to form an integrated infrastructure comprised of three branches. The ELI Beamline Facility (Prague, Czech Republic) will mainly focus on particle acceleration and X-ray generation, while the ELI Nuclear Physics Facility (Magurele, Romania) will be dealing with laser-based nuclear physics as well as high field physics. In the talk we introduce the ELI Attosecond Light Pulse Source (ELI-ALPS) to be built in Szeged, Hungary. The primary mission of the ELI-ALPS Research Infrastructure is to provide the international scientific community with a broad range of ultrafast light sources, especially with coherent XUV and X-ray radiation, including single attosecond pulses. Thanks to this combination of parameters never achieved before, energetic attosecond X-ray pulses of ELI-ALPS will enable recording freeze-frame images of the dynamical electronic-structural behaviour of complex atomic, molecular and condensed matter systems, with attosecond-picometer resolution. The secondary purpose is to contribute to the scientific and technological development towards generating 200 PW pulses, being the ultimate goal of the ELI project. ELI-ALPS will be operated also as a user facility and hence serve basic and applied research in physical, chemical, material and biomedical sciences as well as industrial applications. The Facility will be built by the end of 2015 from a budget exceeding 240M EUR. The building and the IT infrastructure, from high speed internal networking, remote controlled system alignment, targetry and data aquisition through laser and radiation safety tools until security systems, will challenge the state of the art of similar research facilities."
},
{
  "Title": "Software Architecture: Reflections on an Evolving Discipline",
  "Type": "Keynote",
  "Key": "fse25dg",
  "Authors": ["David Garlan", "Mary Shaw"],
  "Affiliations": ["Carnegie Mellon University"],
  "Abstract": "Software Architecture emerged in the 1990s as an important sub-field of software engineering. While good architectural design had long been recognized as critical to the success of any complex software system, before then the practice of architecting had relied largely on ad hoc, uncodified, and idiosyncratic techniques and knowledge. By the 2000s the field had matured to the point where there were widely-recognized taxonomies of architectural patterns, techniques for formally representing and analyzing architectures, methods for reviewing an architectural design, widespread adoption of architectural product lines and composition frameworks, and techniques for ensuring conformance between an architecture and an implementation of it. In this talk we reflect on the key enablers of a discipline of software architecture that led to these advances, the central ideas that form its core, and its enduring principles that continue to shape the field of software engineering. We consider both the important concepts on which it builds, as well as those that have built on top of it. Finally, we examine some of the important new trends and challenges that are likely to have an impact on how software architecture will evolve in the future."
},
{
  "Title": "Social Sensing: When Users Become Monitors",
  "Type": "Paper",
  "Key": "new03a",
  "Authors": ["Raian Ali", "Carlos Solis", "Mazeiar Salehie", "Inah Omoronyia", "Bashar Nuseibeh", "Walid Maalej"],
  "Affiliations": ["Lero - University of Limerick", "Lero - University of Limerick", "Lero \u0013 University of Limerick", "Technische UniverstitÃ¤t MÃ¼nchen"],
  "Abstract": "Adaptation requires a system to monitor its operational context to ensure that when changes occur, a suitable adaptation action is planned and taken at runtime. The ultimate goal of adaptation is that users get their dynamic requirements met efficiently and correctly. Context changes and users' judgment of the role of the system in meeting their requirements are drivers for adaptation. In many cases, these drivers are hard to identify by designers at design time and hard to monitor by the use of exclusively technological means by the system at runtime. In this paper, we propose Social Sensing as the activity performed by users who act as monitors and provide information needed for adaptation at runtime. Such information helps the system cope with technology limitations and designers' uncertainty. We discuss the motivation and foundations of Social Sensing and outline a set of research challenges to address in future work."
},
{
  "Title": "Cross-library API Recommendation using Web Search Engines",
  "Type": "Paper",
  "Key": "new04z",
  "Authors": ["Wujie Zheng", "Qirun Zhang", "Michael Lyu"],
  "Affiliations": ["The Chinese University of Hong Kong"],
  "Abstract": "Software systems are often built upon third party libraries. Developers may replace an old library with a new library, for the consideration of functionality, performance, security, and so on. It is tedious to learn the often complex APIs in the new library from the scratch. Instead, developers may identify the suitable APIs in the old library, and then find counterparts of these APIs in the new library. However, there is typically no such cross-references for APIs in different libraries. Previous work on automatic API recommendation often recommends related APIs in the same library. In this paper, we propose to mine search results of Web search engines to recommend related APIs of different libraries. In particular, we use Web search engines to collect relevant Web search results of a given API in the old library, and then recommend API candidates in the new library that are frequently appeared in the Web search results. Preliminary results of generating related C# APIs for the APIs in JDK show the feasibility of our approach."
},
{
  "Title": "Exploiting Software Architecture to Support RequirementsSatisfaction Testing",
  "Type": "Paper",
  "Key": "new07p",
  "Authors": ["Paul Clements", "Maria Jose Escalona", "Paola Inverardi", "Ivano Malavolta", "Eda Marchetti"],
  "Affiliations": ["Carnegie Mellon University", "University of Sevilla", "Univeristy of L'Aquila", "ISTI-CNR Pisa"],
  "Abstract": "Currently, software testing is mainly carried on independently from software architecture-related information. Some approaches propose to perform integration and regression testing with respect to software architecture descriptions, but less attention has been paid to analysing software architecture in order to develop a less costly and time-consuming test plan that covers the requirements of the system of interest. If on one side, it is well known that providing an effective test plan is crucial to software quality, on the other side software testing is extremely difficult because it stems from the complexity of current software systems. In this paper we (i) elaborate on how a new architectural analysis can help in producing better test plans and (ii) identify a set of corresponding research challenges. We believe that considering and analysing architectural information for requirements satisfaction testing purposes will provide substantial benefits in terms of test plan specification process, test plan effectiveness, and test cases understandability."
},
{
  "Title": "EAGLE: Engineering softwAre in the ubiquitous Globe byLeveraging uncErtainty",
  "Type": "Paper",
  "Key": "new08a",
  "Authors": ["Marco Autili", "Vittorio Cortellessa", "Davide Di Ruscio", "Paola Inverardi", "Patrizio Pelliccione", "Massimo Tivoli"],
  "Affiliations": ["UniversitÃ  dell'Aquila", "UniversitÃ  dell'Aquial"],
  "Abstract": "In the next future we will be surrounded by a virtually infinite number of software applications that provide computational software resources in the open Globe. This will radically change the way software will be produced and used. Users will be keen on producing their own piece of software, by also reusing existing software, to better satisfy their needs, therefore with a goal oriented, opportunistic use in mind. The produced software will need to be able to evolve, react and adapt to a continuously changing environment, while guaranteeing dependability. The strongest adversary to this view is the lack of knowledge on the software\u0019s structure, behavior, and execution context. Despite the possibility to extract observational models from existing software, a producer will always operate with software artifacts that exhibit a degree of uncertainty in terms of their functional and non functional characteristics. We believe that uncertainty can only be controlled by making it explicit and by using it to drive the production process itself. In this paper, we introduce a novel paradigm of software production process that explores available software and assesses its degree of uncertainty in relation to the opportunistic goal G, assists the producer in creating the appropriate integration means towards G, and validates the quality of the integrated system with respect to G and the current context."
},
{
  "Title": "Using Social Media to Study the Diversity of Example Usage among Professional Developers",
  "Type": "Paper",
  "Key": "new10b",
  "Authors": ["Ohad Barzilay", "Orit Hazzan", "Amiram Yehudai"],
  "Affiliations": ["Tel-Aviv University", "Technion - Israel Institute of Technology", "Tel Aviv University"],
  "Abstract": "Socio-professional websites such as LinkedIn use various mechanisms such as network of colleagues, groups and discussions to assist their users in maintaining their professional network and keeping up with relevant discussions. Software professionals post hundreds of thousands of comments each week in these group discussions regarding technological and methodological aspects of their work. Analyzing these comments enables us, the software community at large, to better understand the state of the practice of many aspects of software development. In this paper we describe a case study in which we use such LinkedIn group discussion to learn about developers\u0019 perception of using code examples."
},
{
  "Title": "Introduction to the New Ideas Track",
  "Type": "Paper",
  "Key": "IntroductiontotheNewIdeasTrack",
  "Authors": ["Martin Robillard"]
},
{
  "Title": "Stateful Breakpoints: A Practical Approach to Defining Parameterized Runtime Monitors",
  "Type": "Paper",
  "Key": "new01e",
  "Authors": ["Eric Bodden"],
  "Affiliations": ["Technische UniversitÃ¤t Darmstadt"],
  "Abstract": "A runtime monitor checks a safety property during a program's execution. A parameterized runtime monitor can monitor properties containing free variables, or parameters. For instance, a monitor for the regular expression close(s) read(s)\u001D will warn the user when reading from a stream s that has previously been closed. Parameterized runtime monitors are very expressive, and research on this topic has lately gained much traction in the Runtime Verification community. Existing monitoring algorithms are very efficient. Nevertheless, existing tools provide little support for actually defining runtime monitors, probably one reason for why few practitioners are using runtime monitoring so far. In this work we propose the idea of allowing programmers to express parameterized runtime monitors through stateful breakpoints, temporal combinations of normal breakpoints, a concept well known to programmers. We show how we envision programmers to define runtime monitors through stateful breakpoints and parameter bindings through breakpoint expressions. Further, we explain how stateful break- points improve the debugging experience. they are more expressive than normal breakpoints, nevertheless can be evaluated more efficiently. Stateful breakpoints can be attached to bug reports for easy reproducibility. they often allow developers to run directly to the bug in one single step. Further, stateful breakpoints can potentially be inferred from a running debugging session or using property inference and fault localization tools."
},
{
  "Title": "Inferring Test Results for Dynamic Software Product Lines",
  "Type": "Paper",
  "Key": "new02c",
  "Authors": ["Bruno Cafeo", "Joost Noppen", "Fabiano Ferrari", "Ruzanna Chitchyan", "Awais Rashid"],
  "Affiliations": ["University of Sao Paulo", "University of East Anglia", "Federal University of Sao Carlos", "Lancaster University"],
  "Abstract": "Due to the very large number of configurations that can typically be derived from a Dynamic Software Product Line (DSPL), efficient and effective testing of such systems have become a major challenge for software developers. In particular, when a configuration needs to be deployed quickly due to rapid contextual changes (e.g., in an unfolding crisis), time constraints hinder the proper testing of such a configuration. In this paper, we propose to reduce the testing required of such DSPLs to a relevant subset of configurations. Whenever a need to adapt to an untested configuration is encountered, our approach determines the most similar tested configuration and reuses its test results to either obtain a coverage measure or infer a confidence degree for the new, untested configuration. We focus on providing these techniques for inference of structural testing results for DSPLs, which is supported by an early prototype implementation."
},
{
  "Title": "New Ideas Track: Testing MapReduce-Style Programs",
  "Type": "Paper",
  "Key": "new05c",
  "Authors": ["Christoph Csallner", "Leonidas Fegaras", "Chengkai Li"],
  "Affiliations": ["University of Texas at Arlington"],
  "Abstract": "MapReduce has become a common programming model for processing very large amounts of data, which is needed in a spectrum of modern computing applications. Today several MapReduce implementations and execution systems exist and many MapReduce programs are being developed and deployed in practice. However, developing MapReduce programs is not always an easy task. The programming model makes programs prone to several MapReduce-specific bugs. That is, to produce deterministic results, a MapReduce program needs to satisfy certain high-level correctness conditions. A violating program may yield different output values on the same input data, based on low-level infrastructure events such as network latency, scheduling decisions, etc. Current MapReduce systems and tools are lacking in support for checking these conditions and reporting violations. This paper presents a novel technique that systematically searches for such bugs in MapReduce applications and generates corresponding test cases. The technique works by encoding the high-level MapReduce correctness conditions as symbolic program constraints and checking them for the program under test. To the best of our knowledge, this is the first approach to addressing this problem of MapReduce-style programming."
},
{
  "Title": "Finding Bugs by Isolating Unit Tests",
  "Type": "Paper",
  "Key": "new06k",
  "Authors": ["Kivanc Muslu", "Bilge Soran", "Jochen Wuttke"],
  "Affiliations": ["University of Washington"],
  "Abstract": "Even in simple programs there are hidden assumptions and dependencies between units that are not immediately visible in each involved unit. These dependencies are generally hard to identify and locate, and can lead to subtle faults that are often missed, even by extensive test suites. We propose to leverage existing test suites to identify faults due to hidden dependencies and to identify inadequate test suite design. Rather than just executing entire test suites within frameworks such as JUnit, we execute each test in isolation, thus removing masking effects that might be present in the test suites. We hypothesize that this can reveal previously hidden dependencies between program units or tests. A preliminary study shows that this technique is capable of identifying subtle faults that have lived in a system for 120 revisions, despite failures being reported and despite attempts to fix the fault."
},
{
  "Title": "Join Point Interfaces for Modular Reasoning in Aspect-Oriented Programs",
  "Type": "Paper",
  "Key": "new09i",
  "Authors": ["Milton Inostroza", "Ãric Tanter", "Eric Bodden"],
  "Affiliations": ["University of Chile", "Computer Science Department (DCC)", "Technische UniversitÃ¤t Darmstadt"],
  "Abstract": "While aspect-oriented programming supports the modular definition of crosscutting concerns, most approaches to aspect- oriented programming fail to improve, or even preserve, modular reasoning. The main problem is that aspects usually carry, through their pointcuts, explicit references to the base code. These dependencies make programs fragile. Changes in the base code can unwittingly break a pointcut definition, rendering the aspect ineffective or causing spurious matches. Conversely, a change in a pointcut definition may cause parts of the base code to be advised without notice. Therefore separate development of aspect-oriented programs is largely compromised, which in turns seriously hinders the adoption of aspect-oriented programming by practitioners. We propose to separate base code and aspects using Join Point Interfaces, which are contracts between aspects and base code. Base code can define pointcuts that expose selected join points through a Join Point Interface. Conversely, an aspect can offer to advise join points that pro- vide a given Join Point Interface. Crucially, however, aspect themselves cannot contain pointcuts, and hence cannot refer to base code elements. In addition, because a given join point can provide several Join Point Interfaces, and Join Point Interfaces can be organized in a subtype hierarchy, our approach supports join point polymorphism. We describe a novel advice dispatch mechanism that offers a flexible and type-safe approach to aspect reuse."
},
{
  "Title": "Probabilistic Dataflow Analysis using Path Profiles on Structure Graphs",
  "Type": "Paper",
  "Key": "new11a",
  "Authors": ["Arun Ramamurthi", "Subhajit Roy", "Srikant Y. N"],
  "Affiliations": ["Microsoft India R&D Pvt. Ltd.", "Indian Institute of Technology", "Indian Institute of Science"],
  "Abstract": "Speculative optimizations are increasingly becoming popular for improving program performance by allowing transformations that benefit frequently traversed program paths. Such optimizations are based on dataflow facts which are mostly true, though not always safe. Probabilistic dataflow analysis frameworks infer such facts about a program, while also providing the probability with which a fact is likely to be true. We propose a new Probabilistic Dataflow Analysis Framework which uses path profiles and information about the nesting structure of loops to obtain improved probabilities of dataflow facts."
},
{
  "Title": "How Do Fixes Become Bugs?",
  "Type": "Technical Paper",
  "Key": "esec119",
  "Authors": ["Zuoning Yin", "Ding Yuan", "Yuanyuan Zhou", "Shankar Pasupathy", "Lakshmi Bairavasundaram"],
  "Affiliations": ["University of Illinois at Urbana-Champaign", "University of California, San Diego", "NetApp Inc."],
  "Abstract": "Software bugs affect system reliability. When a bug is exposed in the field, developers need to fix them. Unfortunately, the bug-fixing process can also introduce errors, which leads to buggy patches that further aggravate the damage to end users and erode software vendors' reputation. This paper presents a comprehensive characteristic study on incorrect bug-fixes from large operating system code bases including Linux, OpenSolaris, FreeBSD and also a mature commercial OS developed and evolved over the last 12 years, investigating not only themistake patterns during bug-fixing but also the possible human reasons in the development process when these incorrect bug-fixes were introduced. Our major findings include. (1) at least 14.8%\u0018~24.4% of sampled fixes for post-release bugs in these large OSes are incorrect and have made impacts to end users. (2) Among several common bug types, concurrency bugs are the most difficult to fix correctly. 39% of concurrency bug fixes are incorrect. (3) Developers and reviewers for incorrect fixes usually do not have enough knowledge about the involved code. For example, 27% of the incorrect fixes are made by developers who have never touched the source code files associated with the fix. Our results provide useful guidelines to design new tools and also to improve the development process. Based on our findings, the commercial software vendor whose OS code we evaluated is building a tool to improve the bug fixing and code reviewing process."
},
{
  "Title": "Don't Touch My Code! Examining the Effects of Ownership on Software Quality",
  "Type": "Technical Paper",
  "Key": "esec154",
  "Authors": ["Christian Bird", "Nachiappan Nagappan", "Brendan Murphy", "Harald Gall", "Premkumar Devanbu"],
  "Affiliations": ["Microsoft Research", "University of Zurich", "University of California, Davis"],
  "Abstract": "Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects, Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results."
},
{
  "Title": "ReLink: Recovering Links between Bugs and Changes",
  "Type": "Technical Paper",
  "Key": "esec159",
  "Authors": ["Rongxin Wu", "Hongyu Zhang", "Sunghun Kim", "Shing-Chi Cheung"],
  "Affiliations": ["Tsinghua University", "The Hong Kong University of Science and Technology"],
  "Abstract": "Software defect information, including links between bugs and committed changes, plays an important role in software maintenance such as measuring quality and predicting defects. Usually, the links are automatically mined from change logs and bug reports using heuristics such as searching for specific keywords and bug IDs in change logs. However, the accuracy of these heuristics depends on the quality of change logs. Bird et al. found that there are many missing links due to the absence of bug references in change logs. They also found that the missing links lead to biased defect information, and it affects defect prediction performance. We manually inspected the explicit links, which have explicit bug IDs in change logs and observed that the links exhibit certain features. Based on our observation, we developed an automatic link recovery algorithm, ReLink, which automatically learns criteria of features from explicit links to recover missing links. We applied ReLink to three open source projects. ReLink reliably identified links with 89% precision and 78% recall on average, while the traditional heuristics alone achieve 91% precision and 64% recall. We also evaluated the impact of recovered links on software maintainability measurement and defect prediction, and found the results of ReLink yields significantly better accuracy than those of traditional heuristics."
},
{
  "Title": "High-Impact Defects: A Study of Breakage and Surprise Defects",
  "Type": "Technical Paper",
  "Key": "esec098",
  "Authors": ["Emad Shihab", "Audris Mockus", "Yasutaka Kamei", "Bram Adams", "Ahmed Hassan"],
  "Affiliations": ["Queen's Univeristy", "Avaya Labs Research", "Queen's University"],
  "Abstract": "The relationship between various software-related phenomena (e.g., code complexity) and post-release software defects has been thoroughly examined. However, to date these predictions have a limited adoption in practice. The most commonly cited reason is that the prediction identifies too much code to review without distinguishing the impact of these defects. Our aim is to address this drawback by focusing on high-impact defects for customers and practitioners. Customers are highly impacted by defects that break pre-existing functionality (breakage defects), whereas practitioners are caught off-guard by defects in files that had relatively few pre-release changes (surprise defects). The large commercial software system that we study already had an established concept of breakages as the highest-impact defects, however, the concept of surprises is novel and not as well established. We find that surprise defects are related to incomplete requirements and that the common assumption that a fix is caused by a previous change does not hold in this project. We then fit prediction models that are effective at identifying files containing breakages and surprises. The number of pre-release defects and file size are good indicators of breakages, whereas the number of co-changed files and the amount of time between the latest pre-release change and the release date are good indicators of surprises. Although our prediction models are effective at identifying files that have breakages and surprises, we learn that the prediction should also identify the nature or type of defects, with each type being specific enough to be easily identified and repaired."
},
{
  "Title": "Micro Interaction Metrics for Defect Prediction",
  "Type": "Technical Paper",
  "Key": "esec160",
  "Authors": ["Taek Lee", "Jaechang Nam", "DongGyun Han", "Sunghun Kim", "Hoh In"],
  "Affiliations": ["Korea University", "The Hong Kong University of Science and Technology"],
  "Abstract": "There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy."
},
{
  "Title": "BugCache for Inspections : Hit or Miss?",
  "Type": "Technical Paper",
  "Key": "esec181",
  "Authors": ["Foyzur Rahman", "Daryl Posnett", "Abram Hindle", "Earl Barr", "Premkumar Devanbu"],
  "Affiliations": ["University of California, Davis"],
  "Abstract": "Inspection is a highly effective but costly technique for quality control. Most companies do not have the resources to inspect all the code; thus accurate defect prediction can help focus available inspection resources. BugCache is a simple, elegant, award-winning prediction scheme that \u001Ccaches\u001D files that are likely to contain det. In this paper, we evaluate the utility of BugCache as a tool for focusing inspection, we examine the assumptions underlying BugCache with the aim of improving it, and finally we compare it with a simple, standard bug-prediction technique. We find that BugCache is, in fact, useful for focusing inspection effort; but surprisingly, we find that its performance, when used for inspections, is not much better than a naive prediction model viz, a model that orders files in the system by their count of closed bugs and chooses enough files to capture 20% of the lines in the system."
},
{
  "Title": "Boosting the Performance of Flow-sensitive Points-to Analysis using Value Flow",
  "Type": "Technical Paper",
  "Key": "esec053",
  "Authors": ["Lian Li", "Cristina Cifuentes", "Nathan Keynes"],
  "Affiliations": ["Oracle Labs"],
  "Abstract": "Points-to analysis is a fundamental static analysis technique which computes the set of memory objects that a pointer may point to. Many different applications, such as security-related program analyses, bug checking, and analyses of multi-threaded programs, require precise points-to information to be effective. Recent work has focused on improving the precision of points-to analysis through flow-sensitivity and great progress has been made. However, even with all recent progress, flow-sensitive points-to analysis can still be much slower than a flow-insensitive analysis. In this paper, we propose a novel method that simplifies flow-sensitive points-to analysis to a general graph reachability problem in a value flow graph. The value flow graph summarizes dependencies between pointer variables, including those memory dependencies via pointer dereferences. The points-to set for each pointer variable can then be computed as the set of memory objects that can reach it in the graph. We develop an algorithm to build the value flow graph efficiently by examining the pointed-to-by set of a memory object, i.e., the set of pointers that point to an object. The pointed-to-by information of memory objects is very useful for applications such as escape analysis, and information flow analysis. Our approach is intuitive, easy to implement and very efficient. The implementation is around 2000 lines of code and it is more efficient than existing flow-sensitive points-to analyses. The runtime is comparable with the state-of-the-art flow-insensitive points-to analysis."
},
{
  "Title": "Inferring Data Polymorphism in Systems Code",
  "Type": "Technical Paper",
  "Key": "esec117",
  "Authors": ["Brian Hackett", "Alex Aiken"],
  "Affiliations": ["Stanford University"],
  "Abstract": "We describe techniques for analyzing data polymorphism in C, and show that understanding data polymorphism is important for statically verifying type casts in the Linux kernel, where our techniques prove the safety of 75% of downcasts to structure types, out of a population of 28767. We also discuss prevalent patterns of data polymorphism in Linux, including code patterns we can handle and those we cannot."
},
{
  "Title": "On the Congruence of Modularity and Code Coupling",
  "Type": "Technical Paper",
  "Key": "esec041",
  "Authors": ["Fabian Beck", "Stephan Diehl"],
  "Affiliations": ["University of Trier"],
  "Abstract": "Software systems are modularized to make their inherent complexity manageable. While there exists a set of well-known principles that may guide software engineers to design the modules of a software system, we do not know which principles are followed in practice. In a study based on 16 open source projects, we look at different kinds of coupling concepts between source code entities, including structural dependencies, fan-out similarity, evolutionary coupling, code ownership, code clones, and semantic similarity. The congruence between these coupling concepts and the modularization of the system hints at the modularity principles used in practice. Furthermore, the results provide insights on how to support developers to modularize software systems."
},
{
  "Title": "Fuzzy Set and Cache-based Approach for Bug Triaging",
  "Type": "Technical Paper",
  "Key": "esec132",
  "Authors": ["Ahmed Tamrawi", "Tung Nguyen", "Jafar Al-Kofahi", "Tien Nguyen"],
  "Affiliations": ["Iowa State University"],
  "Abstract": "Bug triaging aims to assign a bug to the most appropriate fixer. That task is crucial in reducing time and efforts in a bug fixing process. In this paper, we propose Bugzie, a novel approach for automatic bug triaging based on fuzzy set and cache-based modeling of the bug-fixing expertise of developers. Bugzie considers a software system to have multiple technical aspects, each of which is associated with technical terms. For each technical term, it uses a fuzzy set to represent the developers who are capable and competent of fixing the bugs relevant to the corresponding aspect. The fixing correlation of a developer toward a technical term is represented by his-her membership score toward the corresponding fuzzy set. The score is calculated based on the bug reports that (s)he has fixed, and is updated as the newly fixed bug reports are available. For a new bug report, Bugzie combines the fuzzy sets corresponding to its terms and ranks the developers based on their membership scores toward that combined fuzzy set to find the most capable fixers. Our empirical results show that Bugzie achieves significantly higher accuracy and time efficiency than existing state-of-the-art approaches."
},
{
  "Title": "Modeling the HTML DOM and Browser API in Static Analysis of JavaScript Web Applications",
  "Type": "Technical Paper",
  "Key": "esec080",
  "Authors": ["Simon Jensen", "Magnus Madsen", "Anders MÃ¸ller"],
  "Affiliations": ["Aarhus University"],
  "Abstract": "Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API. One application of such a static analysis is to detect type-related and dataflow-related programming errors. We report on experiments with a range of modern web applications, including Chrome Experiments and IE Test Drive applications, to measure the precision and performance of the technique. The experiments indicate that the analysis is able to show absence of errors related to missing object properties and to identify dead and unreachable code. By measuring the precision of the types inferred for object properties, the analysis is precise enough to show that most expressions have unique types. By also producing precise call graphs, the analysis additionally shows that most invocations in the programs are monomorphic. We furthermore study the usefulness of the analysis to detect spelling errors in the code. Despite the encouraging results, not all problems are solved and some of the experiments indicate a potential for improvement, which allows us to identify central remaining challenges and outline directions for future work."
},
{
  "Title": "Using an SMT Solver for Interactive Requirements Prioritization",
  "Type": "Technical Paper",
  "Key": "esec088",
  "Authors": ["Francis Palma", "Angelo Susi", "Paolo Tonella"],
  "Affiliations": ["FBK - IRST CIT"],
  "Abstract": "The prioritization of requirements is a crucial activity in the early phases of the software development process. It consists of finding an order relation among requirements, considering several requirements characteristics, such as stakeholder preferences, technical constraints, implementation costs and user perceived value. We propose an interactive approach to the problem of prioritization based on Satisfiability Modulo Theory (SMT) techniques and pairwise comparisons. Our approach resorts to interactive knowledge acquisition whenever the relative priority among requirements cannot be determined based on the available information. Synthesis of the final ranking is obtained via SMT constraint solving. The approach has been evaluated on a set of requirements from a real healthcare project. Results show that it overcomes other interactive state-of-the-art prioritization approaches in terms of effectiveness, efficiency and robustness to decision maker errors."
},
{
  "Title": "CSSL: A Logic for Specifying Conditional Scenarios",
  "Type": "Technical Paper",
  "Key": "esec108",
  "Authors": ["Shoham Ben-David", "Marsha Chechik", "Arie Gurfinkel", "Sebastian Uchitel"],
  "Affiliations": ["University of Toronto", "SEI/CMU", "University of Buenos Aires"],
  "Abstract": "Scenarios and use cases are popular means of describing the intended system behaviour. They support a variety of features and, notably, allow for two different interpretations. existential and universal. These modalities allow a progressive shift from examples to general rules about the expected system behaviour. The combination of modalities in a scenario-based specification poses technical challenges when automated reasoning is to be provided. In particular, the use of conditional existential scenarios, of which use cases with preconditions are a common example, require reasoning in branching time. Yet, formally grounded approaches to requirements engineering and industrial verification approaches shy away from branching-time logics due to their relatively unintuitive semantics. In this paper, we define an extension of an (industry standard) linear-time logic with sufficient branching expressiveness to allow capturing conditional existential statements. The resulting logic, called CSSL (Conditional Scenario Specification Language), has an efficient model-checking procedure. It supports reasoning about heterogeneous requirements specifications that include universal and existential statements in the form of use cases and conditional existential scenarios, and other sequence chart variants, in addition to general (linear) liveness and safety properties. We report on two industrial case studies in which the logic was used to specify and verify scenarios and properties."
},
{
  "Title": "The Onion Patch: Migration in Open Source Ecosystems",
  "Type": "Technical Paper",
  "Key": "esec120",
  "Authors": ["Corey Jergensen", "Anita Sarma", "Patrick Wagstrom"],
  "Affiliations": ["University of Nebraska, Lincoln", "IBM TJ Watson Research Center"],
  "Abstract": "Past research established that individuals joining an Open Source community typically follow a socialization process called \u001Cthe onion model\u001D. newcomers join a project by first contributing at the periphery through mailing list discussions and bug trackers and as they develop skill and reputation within the community they advance to central roles of contributing code and making design decisions. However, the modern Open Source landscape has fewer projects that operate independently and many projects under the umbrella of software ecosystems that bring together projects with common underlying components, technology, and social norms. Participants in such an ecosystems may be able to utilize a significant amount of transferrable knowledge when moving between projects in the ecosystem and, thereby, skip steps in the onion model. In this paper, we examine whether the onion model of joining and progressing in a standalone Open Source project still holds true in large project ecosystems and how the model might change in such settings."
},
{
  "Title": "Does Adding Manpower Also Affect Quality? An Empirical, Longitudinal Analysis",
  "Type": "Technical Paper",
  "Key": "esec126",
  "Authors": ["Andrew Meneely", "Pete Rotella", "Laurie Williams"],
  "Affiliations": ["North Carolina State University", "Cisco Systems, Inc."],
  "Abstract": "With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics, we quantified characteristics of team expansion, including team size, expansion rate, expansion acceleration, and modularity with respect to department designations. We examined statistical correlations between our monthly team-level metrics and monthly product-level metrics. Our results indicate that increased team size and linear growth are correlated with later periods of better product quality. However, periods of accelerated team expansion are correlated with later periods of reduced software quality. Furthermore, our linear regression prediction model based on team metrics was able to predict the product's post-release failure rate within a 95% prediction interval for 38 out of 40 months. Our analysis provides insight for project managers into how the expansion of development teams can impact product quality."
},
{
  "Title": "Effective Communication of Software Development Knowledge Through Community Portals",
  "Type": "Technical Paper",
  "Key": "esec138",
  "Authors": ["Christoph Treude", "Margaret-Anne Storey"],
  "Affiliations": ["University of Victoria"],
  "Abstract": "Knowledge management plays an important role in many software organizations. Knowledge can be captured and distributed using a variety of media, including traditional help files and manuals, videos, technical articles, wikis, and blogs. In recent years, web-based community portals have emerged as an important mechanism for combining various communication channels. However, there is little advice on how they can be effectively deployed in a software project. In this paper, we present a first study of a community portal used by a closed source software project. Using grounded theory, we develop a model that characterizes documentation artifacts along several dimensions, such as content type, intended audience, feedback options, and review mechanisms. Our findings lead to actionable advice for industry by articulating the benefits and possible shortcomings of the various communication channels in a knowledge-sharing portal. We conclude by suggesting future research on the increasing adoption of community portals in software engineering projects."
},
{
  "Title": "Proving Programs Robust",
  "Type": "Technical Paper",
  "Key": "esec049",
  "Authors": ["Swarat Chaudhuri", "Sumit Gulwani", "Roberto Lublinerman", "Sara Navidpour"],
  "Affiliations": ["Rice University", "Microsoft Research", "Pennsylvania State University"],
  "Abstract": "We present a program analysis for verifying quantitative robustness properties of programs, stated generally as. If the inputs of a program are perturbed by an arbitrary amount epsilon, then its outputs change at most by (K . epsilon), where K can depend on the size of the input but not its value. Robustness properties generalize the analytic notion of continuity e.g., while the function e^x is continuous, it is not robust. Our problem is to verify the robustness of a function P that is coded as an imperative program, and can use diverse data types and features such as branches and loops. Our approach to the problem soundly decomposes it into two subproblems (a) verifying that the smallest possible perturbations to the inputs of P do not change the corresponding outputs significantly, even if control now flows along a different control path; and (b) verifying the robustness of the computation along each control-flow path of P. To solve the former subproblem, we build on an existing method for verifying that a program encodes a continuous function. The latter is solved using a static analysis that bounds the magnitude of the slope of any function computed by a control flow path of P. The outcome is a sound program analysis for robustness that uses proof obligations which do not refer to epsilon-changes and can often be fully automated using off-the-shelf SMT-solvers. We identify three application domains for our analysis. First, our analysis can be used to guarantee the predictable execution of embedded control software, whose inputs come from physical sources and can suffer from error and uncertainty. A guarantee of robustness ensures that the system does not react disproportionately to such uncertainty. Second, our analysis is directly applicable to approximate computation, and can be used to provide foundations for a recently-proposed program approximation scheme called loop perforation. A third application is in database privacy proofs of robustness of queries are essential to differential privacy, the most popular notion of privacy for statistical databases."
},
{
  "Title": "Managing Performance vs. Accuracy Trade-offs With Loop Perforation",
  "Type": "Technical Paper",
  "Key": "esec076",
  "Authors": ["Stelios Sidiroglou-Douskos", "Sasa Misailovic", "Henry Hoffmann", "Martin Rinard"],
  "Affiliations": ["MIT"],
  "Abstract": "Many modern computations (such as video and audio encoders, Monte Carlo simulations, and machine learning algorithms) are de- signed to trade off accuracy in return for increased performance. To date, such computations typically use ad-hoc, domain-specific techniques developed specifically for the computation at hand. Loop perforation provides a general technique to trade accu- racy for performance by transforming loops to execute a subset of their iterations. A criticality testing phase filters out critical loops (whose perforation produces unacceptable behavior) to iden- tify tunable loops (whose perforation produces more efficient and still acceptably accurate computations). A perforation space explo- ration algorithm perforates combinations of tunable loops to find Pareto-optimal perforation policies. Our results indicate that, for a range of applications, this approach typically delivers performance increases of over a factor of two (and up to a factor of seven) while changing the result that the application produces by less than 10%."
},
{
  "Title": "Checking Conformance of a Producer and a Consumer",
  "Type": "Technical Paper",
  "Key": "esec167",
  "Authors": ["Evan Driscoll", "Amanda Burton", "Thomas Reps"],
  "Affiliations": ["University of Wisconsin"],
  "Abstract": "This paper addresses the problem of identifying incompatibilities between two programs that operate in a producer/consumer relationship. It describes the techniques that are incorporated in a tool called PCCA (Producer-Consumer Conformance Analyzer), which attempts to (i) determine whether the consumer is prepared to accept all messages that the producer can emit, or (ii) find a counter-example, a message that the producer can emit and the consumer considers ill-formed."
},
{
  "Title": "Fault Localization for Data-Centric Programs",
  "Type": "Technical Paper",
  "Key": "esec015",
  "Authors": ["Diptikalyan Saha", "Mangala Gowri Nanda", "Pankaj Dhoolia", "V. Krishna Nandivada", "Vibha Sinha", "Satish Chandra"],
  "Affiliations": ["IBM Research - India", "IBM T. J. Watson Research Center"],
  "Abstract": "In this paper we present an automated technique for localizing faults in data-centric programs. Data-centric programs primarily interact with databases to get collections of content, process each entry in the collection(s), and output another collection or write it back to the database. One or more entries in the output may be faulty. In our approach, we gather the execution trace of a faulty program. We use a novel, precise slicing algorithm to break the trace into multiple slices, such that each slice maps to an entry in the output collection. We then compute the semantic difference between the slices that correspond to correct entries and those that correspond to incorrect ones. The diff helps to identify potentially faulty statements. We have implemented our approach for ABAP programs. ABAP is the language used to write custom code in SAP systems. It interacts heavily with databases using embedded SQL-like commands that work on collections of data. On a suite of 13 faulty ABAP programs, our technique was able to identify the precise fault location in 12 cases."
},
{
  "Title": "Partial Replay of Long-Running Applications",
  "Type": "Technical Paper",
  "Key": "esec141",
  "Authors": ["Alvin Cheung", "Armando Solar-Lezama", "Samuel Madden"],
  "Affiliations": ["MIT CSAIL"],
  "Abstract": "Bugs in deployed software can be extremely difficult to track down. Invasive logging techniques, such as logging all non-deterministic inputs, can incur substantial runtime overheads. This paper shows how symbolic analysis can be used to re-create path equivalent executions for very long running programs such as databases and web servers. The goal is to help developers debug such long-running programs by allowing them to walk through an execution of the last few requests or transactions leading up to an error. The challenge is to provide this functionality without the high runtime overheads associated with traditional replay techniques based on input logging or memory snapshots. Our approach achieves this by recording a small amount of information about program execution, such as the direction of branches taken, and then using symbolic analysis to reconstruct the execution of the last few inputs processed by the application, as well as the state of memory before these inputs were executed. We implemented our technique in a new tool called bbr. In this paper, we show that it can be used to replay bugs in long-running single-threaded programs starting from the middle of an execution. We show that bbr incurs low recording overhead (avg. of 10%) during program execution, which is much less than existing replay schemes. We also show that it can reproduce real bugs from web servers, database systems, and other common utilities."
},
{
  "Title": "Mitigating the Confounding Effects of Program Dependences for Effective Fault Localization",
  "Type": "Technical Paper",
  "Key": "esec176",
  "Authors": ["George Baah", "Andy Podgurski", "Mary Jean Harrold"],
  "Affiliations": ["Georgia Institute of Technology", "Case Western Reserve University"],
  "Abstract": "Dynamic program dependences are recognized as important factors in software debugging because they contribute to triggering the effects of faults and propagating the effects to a program's output. The effects of dynamic dependences also produce significant confounding bias when statistically estimating the causal effect of a statement on the occurrence of program failures, which leads to poor fault-localization results. This paper presents a novel causal-inference technique for fault localization that accounts for the effects of dynamic data and control dependences and thus, significantly reduces confounding bias during fault localization. The technique employs a new dependence-based causal model together with matching of test executions based on their dynamic dependences. The paper also presents empirical results indicating that the new technique performs significantly better than existing statistical fault-localization techniques as well as our previous fault localization technique based on causal-inference methodology."
},
{
  "Title": "Semistructured Merge: Rethinking Merge in Revision Control Systems",
  "Type": "Technical Paper",
  "Key": "esec006",
  "Authors": ["Sven Apel", "Joerg Liebig", "Benjamin Brandl", "Christian Lengauer", "Christian Kaestner"],
  "Affiliations": ["University of Passau", "Philipps University Marburg"],
  "Abstract": "An ongoing problem in revision control systems is how to resolve conflicts in a merge of independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems that inherit the strengths of both, the generality of unstructured systems and the expressiveness of structured systems. The idea is to provide structural information of the underlying software artifacts, declaratively, in the form of annotated grammars. This way, a wide variety of languages can be supported and the information provided can assist in the automatic resolution of two classes of conflicts, ordering conflicts and semantic conflicts. The former can be resolved independently of the language and the latter using specific conflict handlers. We have been developing a tool that supports semistructured merge and conducted an empirical study on 24 software projects developed in Java, C#, and Python comprising 180 merge scenarios. We found that semistructured merge reduces the number of conflicts in 60% of the sample merge scenarios by, on average, 34%, compared to unstructured merge. We found also that renaming is challenging in that it can increase the number of conflicts during semistructured merge, and that a combination of unstructured and semistructured merge is a pragmatic way to go."
},
{
  "Title": "ADDiff: Semantic Differencing for Activity Diagrams",
  "Type": "Technical Paper",
  "Key": "esec007",
  "Authors": ["Shahar Maoz", "Jan Ringert", "Bernhard Rumpe"],
  "Affiliations": ["RWTH Aachen University"],
  "Abstract": "Activity diagrams (ADs) have recently become widely used in the modeling of workflows, business processes, and web-services, where they serve various purposes, from documentation, requirement definitions, and test case specifications, to simulation and code generation. As models, programs, and systems evolve over time, understanding changes and their impact is an important challenge, which has attracted much research efforts in recent years. In this paper we present addiff, a semantic differencing operator for ADs. Unlike most existing approaches to model comparison, which compare the concrete or the abstract syntax of two given diagrams and output a list of syntactical changes or edit operations, addiff considers the semantics of the diagrams at hand and outputs a set of diff witnesses, each of which is an execution trace that is possible in the first AD and is not possible in the second. We motivate the use of addiff, formally define it, and show two algorithms to compute it, a concrete forward-search algorithm and a symbolic fixpoint algorithm, implemented using BDDs and integrated into the Eclipse IDE. Empirical results and examples demonstrate the feasibility and unique contribution of addiff to the state-of-the-art in version comparison and evolution analysis."
},
{
  "Title": "Proactive Detection of Collaboration Conflicts",
  "Type": "Technical Paper",
  "Key": "esec195",
  "Authors": ["Yuriy Brun", "Reid Holmes", "Michael Ernst", "David Notkin"],
  "Affiliations": ["University of Washington", "University of Waterloo"],
  "Abstract": "Collaborative development can be hampered when conflicts arise because developers have inconsistent copies of a shared project. We present an approach to help developers identify and resolve conflicts early, before those conflicts become severe and before relevant changes fade away in the developers' memories. This paper presents three results. First, a study of open-source systems establishes that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. The study spans nine open-source systems totaling 3.4 million lines of code; our conflict data is derived from 550,000 development versions of the systems. Second, using previously-unexploited information, we precisely diagnose important classes of conflicts using the novel technique of speculative analysis over version control operations. Third, we describe the design of Crystal, a publicly-available tool that uses speculative analysis to make concrete advice unobtrusively available to developers, helping them identify, manage, and prevent conflicts."
},
{
  "Title": "Testing Software In Age Of Data Privacy: A Balancing Act",
  "Type": "Technical Paper",
  "Key": "esec039",
  "Authors": ["Kunal Taneja", "Mark Grechanik", "Rayid Ghani", "Tao Xie"],
  "Affiliations": ["North Carolina State University", "Accenture Technology Labs"],
  "Abstract": "Database-centric applications (DCAs) are common in enterprise computing, and they use nontrivial databases. Testing of DCAs is increasingly outsourced to test centers in order to achieve lower cost and higher quality. When proprietary DCAs are released, their databases should also be made available to test engineers. However, different data privacy laws prevent organizations from sharing this data with test centers because databases contain sensitive information. Currently, testing is performed with anonymized data, which often leads to worse test coverage (such as code coverage) and fewer uncovered faults, thereby reducing the quality of DCAs and obliterating benefits of test outsourcing. To address this issue, we offer a novel approach that combines program analysis with a new data privacy framework that we design to address constraints of software testing. With our approach, organizations can balance the level of privacy with needs of testing. We have built a tool for our approach and applied it to nontrivial Java DCAs. Our results show that test coverage can be preserved at a higher level by anonymizing data based on their effect on corresponding DCAs."
},
{
  "Title": "Strong Higher Order Mutation-Based Test Data Generation",
  "Type": "Technical Paper",
  "Key": "esec090",
  "Authors": ["Mark Harman", "Yue Jia", "William Langdon"],
  "Affiliations": ["University College London"],
  "Abstract": "This paper introduces SHOM, a mutation-based test data generation approach that combines Dynamic Symbolic Execution and Search Based Software Testing. SHOM targets strong mutation adequacy and is capable of killing both first and higher order mutants. We report the results of an empirical study using 17 programs, including production industrial code from ABB and Daimler and open source code as well as previously studied subjects. SHOM achieved higher strong mutation adequacy than two recent mutation-based test data generation approaches, killing between 8% and 38% of those mutants left unkilled by the best performing previous approach."
},
{
  "Title": "Improved Multithreaded Unit Testing",
  "Type": "Technical Paper",
  "Key": "esec194",
  "Authors": ["Vilas Jagannath", "Milos Gligoric", "Dongyun Jin", "Qingzhou Luo", "Grigore Rosu", "Darko Marinov"],
  "Affiliations": ["University of Illinois"],
  "Abstract": "Multithreaded code is notoriously hard to develop and test. A multithreaded test exercises the code under test with two or more threads. Each test execution follows some schedule-interleaving of the multiple threads, and different schedules can give different results. Developers often want to enforce a particular schedule for test execution, and to do so, they use time delays (Thread.sleep in Java). Unfortunately, this approach can produce false positives or negatives, and can result in unnecessarily long testing time. This paper presents IMUnit, a novel approach to specifying and executing schedules for multithreaded tests. We introduce a new language that allows explicit specification of schedules as orderings on events encountered during test execution. We present a tool that automatically instruments the code to control test execution to follow the specified schedule, and a tool that helps developers migrate their legacy, sleep-based tests into event-based tests in IMUnit. The migration tool uses novel techniques for inferring events and schedules from the executions of sleep-based tests. We describe our experience in migrating over 200 tests. The inference techniques have high precision and recall of over 75%, and IMUnit reduces testing time compared to sleep-based tests on average 3.39x."
},
{
  "Title": "Taming Uncertainty in Self-Adaptive Software",
  "Type": "Technical Paper",
  "Key": "esec003",
  "Authors": ["Naeem Esfahani", "Ehsan Kouroshfar", "Sam Malek"],
  "Affiliations": ["George Mason University"],
  "Abstract": "Self-adaptation endows a software system with the ability to satisfy certain objectives by automatically modifying its behavior. While many promising approaches for the construction of self-adaptive software systems have been developed, the majority of them ignore the uncertainty underlying the adaptation decisions. This has been one of the key obstacles to wide-spread adoption of self-adaption techniques in risk-averse real-world settings. In this paper, we describe an approach, called POssIbilistic SElf-aDaptation (POISED), for tackling the challenge posed by uncertainty in making adaptation decisions. POISED builds on possibility theory to assess both the positive and negative consequences of uncertainty. It makes adaptation decisions that result in the best range of potential behavior. We demonstrate POISED's application to the problem of improving a software system's quality of service via runtime reconfiguration of its customizable software components. We have extensively evaluated POISED using a prototype of a robotic software system."
},
{
  "Title": "On Software Component Co-Installability",
  "Type": "Technical Paper",
  "Key": "esec034",
  "Authors": ["Roberto Di Cosmo", "JÃ©rÃ´me Vouillon"],
  "Affiliations": ["Univ Paris Diderot, Sorbonne Paris CitÃ©, PPS, UMR 7126 CNRS", "CNRS, PPS UMR 7126, Univ Paris Diderot, Sorbonne Paris CitÃ©"],
  "Abstract": "Modern software systems are built by composing components drawn from large repositories, whose size and complexity is increasing at a very fast pace. A fundamental challenge for the maintainability and the scalability of such software systems is the ability to quickly identify the components that can or cannot be installed together. this is the co-installability problem, which is related to boolean satisfiability and is known to be algorithmically hard. This paper develops a novel theoretical framework, based on formally certified. semantic preserving graph-theoretic transformations, that allows to associate to each concrete component repository a much smaller one with a simpler structure, but with equivalent co-installability properties. This smaller repository can be represented graphically, giving a concise view of the co-installability issues in the original repository, or used as a basis for various algorithms related to co-installability, like the efficient computation of strong conflicts between components. The proofs contained in this work have been machine checked in Coq."
},
{
  "Title": "Version-consistent Dynamic Reconfiguration of Component-based Distributed Systems",
  "Type": "Technical Paper",
  "Key": "esec121",
  "Authors": ["Xiaoxing Ma", "Luciano Baresi", "Carlo Ghezzi", "Valerio Panzica La Manna", "Jian Lu"],
  "Affiliations": ["Nanjing University", "Politecnico di Milano"],
  "Abstract": "There is an increasing demand for the runtime reconfiguration of distributed systems in response to changing environments and evolving requirements. Reconfiguration must be done in a safe and low-disruptive way. In this paper, we propose version consistency of distributed transactions as a safe criterion for dynamic reconfiguration. Version consistency ensures that distributed transactions be served as if there were operating on a single coherent version of the system despite possible reconfigurations that may happen meanwhile. The paper also proposes a distributed algorithm to maintain dynamic dependences between components at architectural level and enable low-disruptive version-consistent dynamic reconfigurations. An initial assessment through simulation shows the benefits of the proposed approach with respect to timeliness and low degree of disruption."
},
{
  "Title": "Path Exploration based on Symbolic Output",
  "Type": "Technical Paper",
  "Key": "esec071",
  "Authors": ["Dawei Qi", "Hoang Nguyen", "Abhik Roychoudhury"],
  "Affiliations": ["National University of Singapore"],
  "Abstract": "Efficient program path exploration is important for many software engineering activities such as testing, debugging and verification. However, enumerating all paths of a program is prohibitively expensive. In this paper, we develop a partitioning of program paths based on the program output. Two program paths are placed in the same partition if they derive the output similarly, that is, the symbolic expression connecting the output with the inputs is the same in both paths. Our grouping of paths is gradually created by a smart path exploration. Our experiments show the benefits of the proposed path exploration in test-suite construction. Our path partitioning produces a semantic signature of a program, describing all the different symbolic expressions that the output can assume along different program paths. To reason about changes between program versions, we can therefore analyze their semantic signatures. In particular, we demonstrate the applications of our path partitioning in debugging of software regressions."
},
{
  "Title": "Leveraging Existing Instrumentation to Automatically Infer Invariant-Constrained Models",
  "Type": "Technical Paper",
  "Key": "esec096",
  "Authors": ["Ivan Beschastnikh", "Yuriy Brun", "Sigurd Schneider", "Michael Sloan", "Michael Ernst"],
  "Affiliations": ["University of Washington", "Saarland University"],
  "Abstract": "Computer systems are often difficult to debug and understand. A common way of gaining insight into system behavior is to inspect execution logs and documentation. Unfortunately, manual inspection of logs is an arduous process and documentation is often incomplete and out of sync with the implementation. This paper presents Synoptic, a tool that helps developers by inferring a concise and accurate system model. Unlike most related work, Synoptic does not require developer-written scenarios, specifications, negative execution examples, or other complex user input. Synoptic processes the logs most systems already produce and requires developers only to specify a set of regular expressions for parsing the logs. Synoptic has two unique features. First, the model it produces satisfies three kinds of temporal invariants mined from the logs, improving accuracy over related approaches. Second, Synoptic uses refinement and coarsening to explore the space of models. This improves model efficiency and precision, compared to using just one approach. In this paper, we formally prove that Synoptic always produces a model that satisfies exactly the temporal invariants mined from the log, and we argue that it does so efficiently. We empirically evaluate Synoptic through two user experience studies, one with a developer of a large, real-world system and another with 45 students in a distributed systems course. Developers used Synoptic-generated models to verify known bugs, diagnose new bugs, and increase their confidence in the correctness of their systems. None of the developers in our evaluation had a background in formal methods but were able to easily use Synoptic and detect implementation bugs in as little as a few minutes."
},
{
  "Title": "Synthesizing Data Structure Manipulations from Storyboards",
  "Type": "Technical Paper",
  "Key": "esec198",
  "Authors": ["Rishabh Singh", "Armando Solar-Lezama"],
  "Affiliations": ["MIT"],
  "Abstract": "We present the Storyboard Programming framework, a new synthesis system designed to help programmers write imperative low-level data-structure manipulations. The goal of this system is to bridge the gap between the boxes-and-arrows diagrams that programmers often use to think about data-structure manipulation algorithms and the low-level imperative code that implements them. The system takes as input a set of partial input-output examples, as well as a description of the high-level structure of the desired solution. From this information, it is able to synthesize low-level imperative implementations in a matter of minutes. The framework is based on a new approach for combining constraint-based synthesis and abstract-interpretation-based shape analysis. The approach works by encoding both the synthesis and the abstract interpretation problem as a constraint satisfaction problem whose solution defines the desired low-level implementation. We have used the framework to synthesize several data-structure manipulations involving linked lists and binary search trees, as well as an insertion operation into an And Inverter Graph."
},
{
  "Title": "Querypoint : Moving Backwards on Wrong Values in the Buggy Execution",
  "Type": "Tools",
  "Key": "esec301",
  "Authors": ["Salman Mirghasemi", "John Barton", "Claude Petitpierre"],
  "Affiliations": ["ÃCOLE POLYTECHNIQUE FÃDÃRALE DE LAUSANNE", "IBM Research - Almaden"],
  "Abstract": "As developers debug, they often have to seek the origins of wrong values they see in their debugger. This search must be performed backwards in time since the code causing the wrong value is executed before the wrong value appears. Therefore, locating the origin of wrong values with breakpoint- or log- based debuggers demands persistence and significant experience.Querypoint, is a Firefox plugin that enhances the popular Firebug JavaScript debugger with a new, practical feature called lastChange. lastChange automatically locates the last point at which a variable or an object property has been changed. Starting from a program suspended on a breakpoint, the lastChange algorithm applies queries to the live program during re-execution, recording the call stack and limited program state each time the property value changes. When the program halts again on the breakpoint, it shows the call stack and program state at the last change point. To evaluate the usability and effectiveness of Querypoint we studied four experienced JavaScript developers applying the tool to two test cases."
},
{
  "Title": "EvoSuite: Automatic Test Suite Generation for Object-Oriented Software",
  "Type": "Tools",
  "Key": "esec305",
  "Authors": ["Gordon Fraser", "Andrea Arcuri"],
  "Affiliations": ["Saarland University", "Simula Research Laboratory"],
  "Abstract": "To find defects in software, one needs test cases that execute the software systematically, and oracles that assess the correctness of the observed behavior when running these test cases. This paper presents EvoSuite, a tool that automatically generates test cases with assertions for classes written in Java code. To achieve this, EvoSuite applies a novel hybrid approach that generates and optimizes whole test suites towards satisfying a coverage criterion. For the produced test suites, EvoSuite suggests possible oracles by adding small and effective sets of assertions that concisely summarize the current behavior; these assertions allow the developer to detect deviations from expected behavior, and to capture the current behavior in order to protect against future defects breaking this behavior."
},
{
  "Title": "Static Deep Error Checking in Large System Applications using Parfait",
  "Type": "Tools",
  "Key": "esec306",
  "Authors": ["Cristina Cifuentes", "Nathan Keynes", "Lian Li", "Nathan Hawes", "Manuel Valdiviezo", "Andrew Browne", "Jacob Zimmermann", "Andrew Craik", "Douglas Teoh", "Christian Hoermann"],
  "Affiliations": ["Oracle Labs"],
  "Abstract": "In this paper, we introduce Parfait, a static bug-checking tool for C/C++ applications. Parfait achieves precision and scalability at the same time by employing a layered program analysis framework. In Parfait, different analyses varying in precision and runtime expense can be invoked on demand to detect defects of a specific type, effectively achieving higher precision with smaller runtime overheads. Several production organizations within Oracle have started to integrate Parfait into their development process. Feedback from various production teams suggests that it is precise and scalable. the tool is able to analyze the OpenSolarisTM operating system and network consolidation (ON) with more than 6 million lines of code in 1 hour, and report thousands of defects with a false positive rate of close to 10%."
},
{
  "Title": "SMutant: A Tool for Type-Sensitive Mutation Testing in a Dynamic Language",
  "Type": "Tools",
  "Key": "esec308",
  "Authors": ["Milos Gligoric", "Sandro Badame", "Ralph Johnson"],
  "Affiliations": ["University of Illinois at Urbana-Champaign"],
  "Abstract": "A mutation testing tool takes as input a system under test and a test suite and produces as output the mutation score of the test suite. The tool systematically creates mutants by making small syntactic changes to the system under test and executes the test suite to determine which mutants give different results from the original system. Almost all mutation testing tools have been developed for statically typed languages. The lack of tools for dynamically typed languages may be rooted in additional challenges that are caused by the lack of precise type information until the program is executed. Existing tools for dynamically typed languages mostly focus on mutation of literals because the type of literals are known statically. This paper presents SMutant, the first mutation testing tool for Smalltalk programs. In addition to literal replacement, SMutant supports many mutation operators that are commonly seen in tools for statically typed languages, such as operator replacement. Instead of applying mutations statically, SMutant postpones mutating until execution and applies mutations dynamically, when the types are available. Also, SMutant enables the user to define new mutation operators by sending a single message. The tool automatically generates code to support new mutation operators."
},
{
  "Title": "Sydit: Creating and Applying a Program Transformation from an Example",
  "Type": "Tools",
  "Key": "esec310",
  "Authors": ["Na Meng", "Miryung Kim", "Kathryn McKinley"],
  "Affiliations": ["The University of Texas at Austin", "The University of Texas at Austin & Microsoft Research"],
  "Abstract": "Bug fixes and feature additions to large code bases often require systematic edits\u0014similar, but not identical, coordi- nated changes to multiple places. This process is tedious and error-prone. Our prior work introduces a systematic editing approach that creates generalized edit scripts from exem- plar edits and applies them to user-selected targets. This paper describes how the Sydit plug-in integrates our tech- nology into the Eclipse integrated development environment. A programmer provides an example edit to Sydit that con- sists of an old and new version of a changed method. Based on this one example, Sydit generates a context-aware, ab- stract edit script. To make transformations applicable to similar but not identical methods, Sydit encodes control, data, and containment dependences and abstracts position, type, method, and variable names. Then the programmer selects target methods and Sydit customizes the edit script to each target and displays the results for the programmer to review and approve. Sydit thus automates much of the systematic editing process. To fully automate systematic editing, future tool enhancements should include automated selection of targets and testing of Sydit generated edits."
},
{
  "Title": "jStar-eclipse: An IDE for Automated Verification of Java Programs",
  "Type": "Tools",
  "Key": "esec313",
  "Authors": ["Daiva Naudziuniene", "Matko Botincan", "Dino Distefano", "Mike Dodds", "Radu Grigore", "Matthew Parkinson"],
  "Affiliations": ["University of Cambridge", "Queen Mary University of London & Monoidics Ltd", "Queen Mary University of London", "Microsoft Research"],
  "Abstract": "jStar is a tool for automatically verifying Java programs. It uses separation logic to support abstract reasoning about object specifications. jStar can verify a number of challenging design patterns, including Subject Observer, Visitor, Factory and Pooling. However, to use jStar one has to deal with a family of command-line tools that expect specifications in separate files and diagnose the errors by inspecting the text output from these tools. In this paper we present a plug-in, called jStar-eclipse, allowing programmers to use jStar from within Eclipse IDE. Our plug-in allows writing method contracts in Java source files in form of Java annotations. It automatically translates Java annotations into jStar specifications and propagates errors reported by jStar back to Eclipse, pinpointing the errors to the locations in source files. This way the plug-in ensures an overall better user experience when working with jStar. Our end goal is to make automated verification based on separation logic accessible to a broader audience."
},
{
  "Title": "SCORE: a Scalable Concolic Testing Tool for Reliable Embedded Software",
  "Type": "Tools",
  "Key": "esec314",
  "Authors": ["Yunho Kim", "Moonzoo Kim"],
  "Affiliations": ["Korea Advanced Institute of Science and Technology"],
  "Abstract": "Current industrial testing practices often generate test cases in a manual manner, which degrades both the effectiveness and efficiency of testing. To alleviate this problem, concolic testing generates test cases that can achieve high coverage in an automated fashion. One main task of concolic testing is to extract symbolic information from a concrete execution of a target program at runtime. Thus, a design decision on how to extract symbolic information affects efficiency, effectiveness, and applicability of concolic testing. We have developed a Scalable COncolic testing tool for REliable embedded software (SCORE) that targets embedded C programs. SCORE instruments a target C program to extract symbolic information and applies concolic testing to a target program in a scalable manner by utilizing a large number of distributed computing nodes. In this paper, we describe our design decisions that are implemented in SCORE and demonstrate the performance of SCORE through the experiments on the SIR benchmarks."
},
{
  "Title": "Cross-Layer Modeler - A Tool for Flexible Multilevel Modeling with Consistency Checking",
  "Type": "Tools",
  "Key": "esec302",
  "Authors": ["Andreas Demuth", "Roberto Lopez-Herrejon", "Alexander Egyed"],
  "Affiliations": ["Johannes Kepler University"],
  "Abstract": "Model-driven engineering has become a popular methodology in software engineering. Most available modeling tools support the creation of models based on a fixed metamodel. Typically, tool users cannot change the metamodel to reflect domain changes or newly emerged requirements. As a consequence, an updated version of the tool with an evolved metamodel must be developed and models as well as constraints that ensure model consistency have to be co-evolved, often manually, to conform to the new metamodel. Both, tool evolution and the necessary co-evolutions, are time consuming and error prone tasks. Furthermore, common tools often restrict the number of metalevels that can be modeled and force modelers to use workarounds to express certain facts. To overcome these issues we present the Cross-Layer Modeler (XLM), a modeling tool that supports multilevel modeling and allows co-evolution of metamodels and models. The XLM automatically performs co-evolution of constraints and gives instant feedback about model consistency. We illustrate the novel modeling approach of our tool and discuss its main capabilities."
},
{
  "Title": "SafeSlice: A Model Slicing and Design Safety Inspection Tool for SysML",
  "Type": "Tools",
  "Key": "esec303",
  "Authors": ["Davide Falessi", "Shiva Nejati", "Mehrdad Sabetzadeh", "Lionel Briand", "Antonio Messina"],
  "Affiliations": ["University of Rome (Tor Vergata)", "Simula Research Lab"],
  "Abstract": "Software safety certification involves checking that the software design meets the (software) safety requirements. In practice, inspections are one of the primary vehicles for ensuring that safety requirements are satisfied by the design. Unless the safety-related aspects of the design are clearly delineated, the inspections conducted by safety assessors would have to consider the entire design, although only small fragments of the design may be related to safety. In a model-driven development context, this means that the assessors have to browse through large models, understand them, and identify the safety-related fragments. This is time-consuming and error-prone, specially noting that the assessors are often third-party regulatory bodies who were not involved in the design. To address this problem, we describe in this paper a prototype tool called, SafeSlice, that enables one to automatically extract the safety-related slices (fragments) of design models. The main enabler for our slicing technique is the traceability between the safety requirements and the design, established by following a structured design methodology that we propose. Our work is grounded on SysML, which is being increasingly used for expressing the design of safety-critical systems. We have validated our work through two case studies and a control experiment which we briefly outline in the paper."
},
{
  "Title": "Tool Support for UML-Based Specification and Verification of Role-Based Access Control Properties",
  "Type": "Tools",
  "Key": "esec304",
  "Authors": ["Lionel Montrieux", "Michel Wermelinger", "Yijun Yu"],
  "Affiliations": ["The Open University"],
  "Abstract": "It has been argued that security perspectives, of which access control is one, should be taken into account as early as possible in the software development process. Towards that goal, we present in this paper a tool supporting our modelling approach to specify and verify access control in accordance to the NIST standard Role-Based Access Control (RBAC). RBAC is centred on mapping users to their roles in an organisation, to make access control permissions easier to set and maintain. Our modelling approach uses only standard UML mechanisms, like metamodels and OCL constraints, and improves on existing approaches in various ways, designers don't have to learn new languages or adopt new tools or methodologies; user-role and role-permission assignments can be specified separately to be reused across models; access control is specified over class and activity diagrams, including anti-scenarios; access control is automatically verified. The tool is built on top of an existing modelling IDE and allows for automatic verification of models according to our RBAC modelling approach, while providing users with the ability to easily identify and correct errors in the model when they are detected."
},
{
  "Title": "Design and Validation of Feature-based Process Model Tailoring",
  "Type": "Tools",
  "Key": "esec307",
  "Authors": ["Daniela Costache", "Georg Kalus", "Marco Kuhrmann"],
  "Affiliations": ["Technische UniversitÃ¤t MÃ¼nchen"],
  "Abstract": "A comprehensive software development process needs some adjustment before it can be used. It needs to be tailored to the particular organization's and project's setting. The definition of an appropriate tailoring model is a critical task. Process users need tailoring that enables them to trim the process to reflect the actual needs. Process engineers need a method and a tool to define a valid model. The SE Book of T-Systems contains a feature model to describe variable parts of the process model and relations and constraints between these parts. The notation and semantics of feature models can be used to visually author a consistent and valid tailoring model. In this paper we present a tool for visual modeling and validation of process model tailoring based on feature models using the SE Book of T-Systems as an example. The tool is based on a domain-specific language that represents the process model. It leverages the semantics of feature models to provide an easy-to-use editor for tailoring-enabled process models."
},
{
  "Title": "Synoptic: Studying Logged Behavior with Inferred Models",
  "Type": "Tools",
  "Key": "esec309",
  "Authors": ["Ivan Beschastnikh", "Jenny Abrahamson", "Yuriy Brun", "Michael Ernst"],
  "Affiliations": ["University of Washington"],
  "Abstract": "Logging is a powerful method for capturing program activity and state during an execution. However, log inspection remains a tedious activity, with developers often piecing together what went on from multiple log lines and across many files. This paper describes Synoptic, a tool that takes logs as input and outputs a finite state machine that models the process generating the logs. The paper overviews the model inference algorithms. Then, it describes the Synoptic tool, which is designed to support a rich log exploration workflow."
},
{
  "Title": "PSPWizard: Machine-assisted Definition of Temporal Logical Properties with Specification Patterns",
  "Type": "Tools",
  "Key": "esec311",
  "Authors": ["Markus Lumpe", "Indika Meedeniya", "Lars Grunske"],
  "Affiliations": ["Swinburne University of Technology", "University of Kaiserslautern"],
  "Abstract": "Model checking provides a powerful means to assert and verify desired system properties. But, for the verification process to become feasible, a correct formulation of these properties in a temporal logic is necessary - a potential barrier to application in practice. Research on property specification has supplied us with rich pattern catalogs that capture commonly occurring system properties in different temporal logics. Furthermore, these property specification pattern catalogs usually offer both a structured English grammar to facilitate the pattern selection and an associated template solutions to express the properties formally. Yet, the actual use of property specification patterns remains cumbersome, due to limited tool support. For this reason, we have developed the Property Specification Pattern Wizard (PSPWizard), a framework that defines an interface for the currently accepted property specification pattern libraries. PSPWizard consists of two main building blocks. a mapping generator that weaves a given pattern library with a target logic and a GUI front-end to the structured English grammar tailored to those patterns that are supported in the target logic."
},
{
  "Title": "Crystal: Precise and Unobtrusive Conflict Warnings",
  "Type": "Tools",
  "Key": "esec312",
  "Authors": ["Yuriy Brun", "Reid Holmes", "Michael Ernst", "David Notkin"],
  "Affiliations": ["University of Washington", "University of Waterloo"],
  "Abstract": "During collaborative development, individual developers can create conflicts in their copies of the code. Such conflicting edits are frequent in practice, and resolving them can be costly. We present Crystal, a tool that proactively examines developers' code and precisely identifies and reports on textual, compilation, and behavioral conflicts. When conflicts are present, Crystal enables developers to resolve them more quickly, and therefore at a lesser cost. When conflicts are absent, Crystal increases the developers'confidence that it is safe to merge their code. Crystal uses an unobtrusive interface to deliver pertinent information about conflicts. It informs developers about actions that would address the conflicts and about people with whom they should communicate."
},
{
  "Title": "A Process for Assessing Data Quality",
  "Type": "Workshop Paper",
  "Key": "wosq01s",
  "Authors": ["Harry Sneed", "Rudolf Majnar"],
  "Affiliations": ["ANECON GmbH", "SoRing Kft."],
  "Abstract": "Abstract. This industrial report stems from practical experience in assessing the quality of customer databases. The process it describes unites three automated audits, - an audit of the database schema, an audit of the database structure and an audit of the database content. The audit of the database schema checks for design smells and rule violations. The audit of the database structure measures the size, complexity and quality of the database model. The audit of the database content processes the data itself to uncover invalid data values, missing records and redundant records. The purpose of these audits is to assess the quality of the database and to determine whether a data reengineering or data clean-up project is required."
},
{
  "Title": "A Unifying Model for Software Quality",
  "Type": "Workshop Paper",
  "Key": "wosq02k",
  "Authors": ["Klaus Lochmann", "Andreas Goeb"],
  "Affiliations": ["Technische UniversitÃ¤t MÃ¼nchen", "SAP Research"],
  "Abstract": "Assuring high quality of software is crucial, but a highly complex topic. It is intermingled with most disciplines of software engineering, which have developed their own quality assurance approaches. However, they lack a common foundation, which leads to loss of information between the disciplines and requires additional tracking effort. There is no comprehensive framework to describe all different concepts relating to software quality in a common way. In this paper we present a general quality model, providing the possibility to describe very different concepts related to quality. We show that our quality model is able to integrate the various concepts found in standards, quality models, guidelines, and static code checker rules. Furthermore, we show that the quality model is able to describe the interrelations of disciplines, like requirements engineering and software test, to software quality. With this quality model, we provide a common foundation for concepts related to software quality, enabling consistency and continuity of quality-related information during software development."
},
{
  "Title": "Do Software Process Improvements Lead to ISO 9126 Architectural Quality Factor Improvement?",
  "Type": "Workshop Paper",
  "Key": "wosq04m",
  "Authors": ["Mathieu LavallÃ©e", "Pierre Robillard"],
  "Affiliations": ["Ãcole Polytechnique de MontrÃ©al"],
  "Abstract": "This paper presents preliminary results of a systematic review performed to determine the impacts of Software Process Improvements (SPI) on developers' activities and on architectural quality. The analysis shows that most SPI research focuses on the motivations of developers like quality of work life and participation incentives, but provides little detail on the impacts of SPI on their day-to-day tasks. The impacts on product quality are limited to defect reduction, and do not consider architectural quality factors, such as changeability and stability. This study shows a very weak link between process quality, as defined by the CMMI, and architectural quality, as defined by ISO 9126. The SPI literature found by this review is mostly concerned with requirement process improvements, which are related to problem definition quality, but not to architectural quality. Future quality-oriented SPI research should therefore focus on improving design and development processes with an eye to considering architectural quality factors, or what the ISO 9126 terms \u001Carchitectural capabilies."
},
{
  "Title": "Introduction of Japan's Investigation Activities on Systems and Software Product Quality Metrics",
  "Type": "Workshop Paper",
  "Key": "wosq05y",
  "Authors": ["Masae Yamamuro", "Yukio Tanitsu", "Toshihiro Komiyama", "Motoei Azuma"],
  "Affiliations": ["Mitsubishi Research Institute, Inc.", "IBM Japan, Ltd.", "NEC Corporation", "Waseda University"],
  "Abstract": "In order to realize an environment where safe and secure system/software products can be used in daily life and social economic activities, it is required to visualize the quality of the product, evaluate whether it meets the user's needs objectively and achieve quality target. So, the Ministry of Economy, Trade and Industry, Japan (METI), Advanced Research Project on Software Metrics - Product Quality Metrics Working Group (WG) has worked on establishing metrics (the term \u001Cmetric\u001D is used to refer to base measures, derived measures and indexes as a batch. In this paper, metric is used to describe the measured volume.) that can be used commonly for these activities and summarized the contents in order to promote quality assurance activity. First of all the WG organized discussions related to metrics in order to clarify the quality of various system/software products existing in Japan, and aggregated characteristics of metrics and mutual relationship.Then, for the objective of establishing the quality of system/software products meeting the users' needs and to select metrics that can be used commonly for establishment, the in-depth research was performed and investigation was held for basic activities from quality requirement definition to quality evaluation and metrics recommended for use for each quality characteristic. This paper described the pertinent contents."
},
{
  "Title": "An Explanatory Analysis on Eclipse Beta-Release Bugs Through In-Process Metrics",
  "Type": "Workshop Paper",
  "Key": "wosq07a",
  "Authors": ["Ayse Tosun Misirli", "Brendan Murphy", "Thomas Zimmermann", "Ayse Basar Bener"],
  "Affiliations": ["Bogazici University", "Microsoft Research", "Ryerson University"],
  "Abstract": "Failures after the release of software products are expensive and time-consuming to fix. Each of these failures has different reasons pointing into different portions of code. We conduct a retrospective analysis on bugs reported after beta release of Eclipse versions. Our objective is to investigate what went wrong during the development process. We identify six in-process metrics that have explanatory effects on beta-release bugs. We conduct statistical analyses to check relationships between files and metrics. Our results show that files with beta-release bugs have different characteristics in terms of in-process metrics. Those bugs are specifically concentrated on Eclipse files with little activity, few edits by few committers. We suggest that in-process metrics should be investigated individually to identify beta-release bugs. Companies may benefit from such a retrospective analysis to understand characteristics of failures. Corrective actions can be taken earlier in the process to avoid similar failures in future releases."
},
{
  "Title": "The Use of Application Scanners in Software Product Quality Assessment",
  "Type": "Workshop Paper",
  "Key": "wosq09s",
  "Authors": ["Stefan Wagner"],
  "Affiliations": ["University of Stuttgart"],
  "Abstract": "Software development needs continuous quality control for a timely detection and removal of quality problems. This includes frequent quality assessments, which need to be au- tomated as far as possible to be feasible. One way of automation in assessing the security of software are application scanners that test an executing software for vulnerabilities. At present, common quality assessments do not integrate such scanners for giving an overall quality statement. This paper presents an integration of application scanners into a general quality assessment method based on explicit quality models and Bayesian nets. Its applicability and the detection capabilities of common scanners are investigated in a case study with two open-source web shops."
},
{
  "Title": "A Software Quality Model for SOA",
  "Type": "Workshop Paper",
  "Key": "wosq10g",
  "Authors": ["Andreas Goeb", "Klaus Lochmann"],
  "Affiliations": ["SAP Research", "Technische UniversitÃ¤t MÃ¼nchen"],
  "Abstract": "Service-oriented architectures (SOAs) are well established as an architectural paradigm for distributed systems. With software systems becoming more and more complex over time, quality assurance becomes increasingly important. A clear understanding of software quality for SOA is therefore crucial in order to assure quality in the long run. In this paper, we present a unifying meta-model to describe the quality of service-oriented systems as an enhancement of the Quamoco meta-model. To put these modeling concepts into practice, we present examples from an initial quality model for SOA-based systems, which is based on empirical results from other sources in the SOA quality community. By integrating these sources of information, similarities as well as contradictions within and between the various models for SOA quality are made transparent. This is the baseline for defining a comprehensive SOA quality model. In addition, this approach represents SOA's distinguishing features regarding quality modeling as first-class model entities to reduce modeling effort while increasing model expressiveness."
},
{
  "Title": "The Impact of ICT Evolution and Application Explosion on Software Quality",
  "Type": "Keynote",
  "Key": "wosq11ma",
  "Authors": ["Motoei AZUMA"],
  "Affiliations": ["Waseda University"]
},
{
  "Title": "When the Requirements for Adaptation and High Integrity Meet",
  "Type": "Workshop Paper",
  "Key": "asas01rc",
  "Authors": ["Radu Calinescu"],
  "Affiliations": ["Aston University"],
  "Abstract": "Two classes of software that are notoriously difficult to develop on their own are rapidly merging into one. This will affect every key service that we rely upon in modern society, yet a successful merge is unlikely to be achievable using software development techniques specific to either class.This paper explains the growing demand for software capable of both self-adaptation and high integrity, and advocates the use of a collection of runtime techniques for its development, operation and management. We summarise early research into the development of such techniques, and discuss the remaining work required to overcome the great challenge of self-adaptive high-integrity software."
},
{
  "Title": "Towards Accurate Failure Prediction for the Proactive Adaptation of Service-oriented Systems",
  "Type": "Workshop Paper",
  "Key": "asas04am",
  "Authors": ["Andreas Metzger"],
  "Affiliations": ["University of Duisburg-Essen"],
  "Abstract": "Furnishing service-oriented systems with self-adaptation capabilities allows those systems to become resilient against failures of their constituent services. Especially proactive adaptation capabilities, which strive to prevent the impacts of pending failures, provide significant benefits, such as avoiding costly compensation and repair activities. An important challenge is to trigger proactive adaptations accurately; firstly, because executing unnecessary proactive adaptations can lead to additional costs or failures that would not have arisen in the non-adapted systems; secondly, because missed proactive adaptation opportunities diminish the benefits of such adaptations. This paper discusses two directions along which accurate proactive adaptations can be achieved, (i) by improving the failure prediction techniques that trigger the adaptations (i.e., during design time); (ii) by dynamically estimating the accuracy of the predicted failures during the operation of the service-oriented system (i.e., during run-time). The discussion is backed by concrete examples of existing prediction techniques for service-oriented systems and supported by experimental results."
},
{
  "Title": "Component-based Timed Hazard Analysis of Self-healing Systems",
  "Type": "Workshop Paper",
  "Key": "asas07c",
  "Authors": ["Claudia Priesterjahn", "Dominik Steenken", "Matthias Tichy"],
  "Affiliations": ["University of Paderborn", "University of Augsburg"],
  "Abstract": "Today, self-healing is increasingly used in embedded real-time systems, that are applied in safety-critical environments, to reduce hazards. These systems implement self-healing by reconfiguration, i.e., the exchange of system components during run-time that aims at stopping or removing failures. This reaction is subject to hard real-time constraints because reacting too late does not yield the intended effects. Consequently, it is necessary to analyze the propagation of failures over time and also take into account how the propagation of failures is changed by the reconfiguration. Current approaches do not analyze the propagation times of failures and the changes of structural reconfiguration on the failure propagation. We enhance our hazard analysis approach by extending our failure propagation models by propagation times and taking the system's real-time reconfiguration behavior into account. This allows to analyze how a reconfiguration with certain duration changes the failure propagation of a real-time system and thus whether it is able to prevent a hazard. We show the feasibility of our approach by an example case study from the RailCab project."
},
{
  "Title": "Using Feature Locality: Can We Leverage History to Avoid Failures During Reconfiguration?",
  "Type": "Workshop Paper",
  "Key": "asas10b",
  "Authors": ["Brady Garvin", "Myra Cohen", "Matthew Dwyer"],
  "Affiliations": ["University of Nebraska-Lincoln"],
  "Abstract": "Despite the best efforts of software engineers, faults still escape into deployed software. Developers need time to prepare and distribute fixes, and in the interim deployments must either tolerate or avoid failures. Self-adaptive systems, systems that adapt to meet changing requirements in a dynamic environment, have a daunting task if their reconfiguration involves adding or removing functional features, because configurable software is known to suffer from failures that appear only under certain feature combinations. Although configuration-dependent failures may be difficult to provoke, and thus hard to detect in testing, we posit that they also constitute opportunities for reconfiguration to increase system reliability. We further conjecture that the failures that are sensitive to a system configuration depend on similar feature combinations, a phenomenon we call feature-locality, and that this locality can be combined with historical data to predict failure-prone configurations. In a case study on 128 failures reported against released versions of an open source configurable system, we find evidence to support our hypothesis. We show that only a small number of features affect the visibility of these failures, and that over time we can learn these features to avoid future failures."
},
{
  "Title": "Robust-and-evolvable Resilient Software Systems",
  "Type": "Workshop Paper",
  "Key": "asas13d",
  "Authors": ["Vincenzo De Florio"],
  "Affiliations": ["Universiteit Antwerpen"],
  "Abstract": "How to build robust-and-evolvable resilient software systems? On the one hand, evolvability implies a system's ability to reach autonomously new, possibly unprecedented conditions and states; on the other hand, resilience and robustness refer to feature persistence in the face of perturbations and changes such as the ones brought about by system evolution. How to address such problem? How to make sure that a software system shall exhibit emerging properties such as reliability or safety irrespective of its environmental conditions and whatever its new configuration may be? In this paper we discuss these problems leveraging from three cases of adaptive software and systems designs. Our conclusions are that the complexity and characteristics of the system components in control of evolution as well as the forms of interaction of the systems with their environments play a key role in the ultimate emergence of sought properties and behaviours. Moreover we highlight how social eco-systems in heterarchical organisation appear to have a stronger resilience and robustness throughout their evolutions."
},
{
  "Title": "Model Checking Requirements at Run-time in Adaptive Systems",
  "Type": "Workshop Paper",
  "Key": "asas16sp",
  "Authors": ["Paola Inverardi", "Marco Mori"],
  "Affiliations": ["UniversitÃ  dell'Aquila", "IMT Institute for Advanced Studies Lucca"],
  "Abstract": "Environment uncertainty opens new scenarios to the problem of assessing modern software systems. To this end, requirements have been studied to guide system evolution despite continuous context variations. At design-time, the software designer provides a set of requirements specifications to assess system functionalities. These functionalities are strictly related to the context in which the system operates. If the context changes unpredictably the user may require new behaviors at run-time in terms of new requirements and corresponding implementations that have to be deployed. Since these requirements are not known at design-time the adapted implementation should be dynamically validated before delivering new functionalities to the user. Following a feature engineering perspective we propose a framework to support correct unanticipated evolutions of requirements. Our process validates evolved implementations artifacts with respect to evolved requirements specifications."
},
{
  "Title": "RV: A Runtime Verification Framework for Monitoring, Prediction and Mining",
  "Type": "Keynote",
  "Key": "RVARuntimeVerificationFrameworkforMonitoringPredictionandMining",
  "Authors": ["Grigore Rosu"],
  "Affiliations": ["University of Illinois at Urbana-Champaign"],
  "Abstract": "Runtime verification is an emerging approach which consists of observing the execution of the target system, checking the observed execution trace against the requirements, and steering the running program to avoid catastrophic failures if needed. Runtime verification avoids the complexity of full-blown formal verification by analyzing a model extracted from the execution of the program rather than from the program itself. It also avoids the ad hoc nature of testing by taking formal specifications and analyzing models (extracted from traces) which may be more complex than linear sequences of events. The current limitations of runtime verification are the following, runtime and memory overhead - to observe a program one needs to instrument it and to manage a potentially unbounded number the monitors; limited coverage - there could be errors in the program which are not encountered in the observed execution trace; and need for formal specifications - to state the correctness criteria, one still needs to provide formal specifications. RV is a runtime verification framework currently under development by a startup company jointly with the Formal Systems Laboratory at UIUC. The objective of RV is to circumvent the current limitations of runtime verification. The runtime and memory overhead are addressed by developing efficient instrumentation and monitor garbage collection.The limited coverage is addressed by extracting causal models from execution traces and analyzing them efficiently; this way, a causal model may predict an error in the running system even though the error has not been hit during the observed execution. Finally, to relieve users from writing formal specifications, an automated specification miner observes execution traces and learns the most likely formal specifications that they obey. The ultimate goal of RV is to not only allow its users to employ monitoring, prediction and mining in their systems, but also to combine them in ingenious ways. For example, one can completely automatically mine likely specifications and then use prediction on them to detect potential errors. Or one can mine properties of a library using traces from trusted programs, and then use the mined properties to monitor or predict errors in untrusted programs. RV extends and combines technologies developed under the UIUC projects JavaMOP (OOPSLA'07, TACAS'09, PLDI'11), jPredictor (CAV'07, ICSE'08) and jMiner (ICSE'11)."
},
{
  "Title": "A Taxonomy for Software Change Impact Analysis",
  "Type": "Workshop Paper",
  "Key": "iwpse07",
  "Authors": ["Steffen Lehnert"],
  "Affiliations": ["Ilmenau University of Technology"],
  "Abstract": "Most software is accompanied by frequent changes, whereas the implementation of a single change can affect many different parts of the system. Approaches for Impact Analysis have been developed to assist developers with changing software. However, there is no solid framework for classifying and comparing such approaches, and it is therefore hard to find a suitable technique with minimal effort. The contribution of this paper is a taxonomy for Impact Analysis, based on a literature review conducted on related studies, to overcome this limitation. The presented classification criteria are more detailed and precise than those proposed in previous work, and possible candidates for all criteria are derived from studied literature. We classify several approaches according to our taxonomy to illustrate its applicability and the usefulness of our criteria. The research presented in this paper prepares the ground for a comprehensive survey of Software Change Impact Analysis."
},
{
  "Title": "Run-time Phenomena in Dynamic Software Updating: Causes and Effects",
  "Type": "Workshop Paper",
  "Key": "iwpse10",
  "Authors": ["Allan Gregersen", "Bo JÃ¸rgensen"],
  "Affiliations": ["University of Southern Denmark"],
  "Abstract": "The development of a dynamic software updating system for statically-typed object-oriented programming languages has turned out to be a challenging task. Despite the fact that the present state of the art in dynamic updating systems, like JRebel, Dynamic Code Evolution VM, JVolve and Javeleon, all provide very transparent and flexible technical solutions to dynamic updating, case studies have shown that designing dynamically updatable applications still remains a challenging task. This challenge has its roots in a number of run-time phenomena that are inherent to dynamic updating of applications written in statically-typed object-oriented programming languages. In this paper, we present our experience from developing dynamically updatable applications using a state-of-the-art dynamic updating system for Java. We believe that the findings presented in this paper provide an important step towards a better understanding of the implications of dynamic updating on the application design."
},
{
  "Title": "Towards a Benchmark for Traceability",
  "Type": "Workshop Paper",
  "Key": "iwpse22",
  "Authors": ["Eya Ben Charrada", "David Caspar", "CÃ©dric Jeanneret", "Martin Glinz"],
  "Affiliations": ["Department of Informatics, University of Zurich"],
  "Abstract": "Rigorously evaluating and comparing traceability link generation techniques is a challenging task. In fact, traceability is still expensive to implement and it is therefore difficult to find a complete case study that includes both a rich set of artifacts and traceability links among them. Consequently, researchers usually have to create their own case studies by taking a number of existing artifacts and creating traceability links for them. There are two major issues related to the creation of one's own example. First, creating a meaningful case study is time consuming. Second, the created case usually covers a limited set of artifacts and has a limited applicability (e.g., a case with traces from high-level requirements to low-level requirements cannot be used to evaluate traceability techniques that are meant to generate links from documentation to source code). We propose a benchmark for traceability that includes all artifacts that are typically produced during the development of a software system and with end-to-end traceability linking. The benchmark is based on an irrigation system that was elaborated in a book about software design. The main task considered by the benchmark is the generation of traceability links among different types of software artifacts. Such a traceability benchmark will help advance research in this field because it facilitates the evaluation and comparison of traceability techniques and makes the replication of experiments an easy task. As a proof of concept we used the benchmark to evaluate the precision and recall of a link generation technique based on the vector space model. Our results are comparable to those obtained by other researchers using the same technique."
},
{
  "Title": "Towards a Classification of Logical Dependencies Origins: A Case Study",
  "Type": "Workshop Paper",
  "Key": "iwpse25",
  "Authors": ["Gustavo Oliva", "Francisco Santana", "Marco Gerosa", "Cleidson de Souza"],
  "Affiliations": ["University of SÃ£o Paulo (USP)", "Federal University of ParÃ¡ (UFPA)", "IBM Research"],
  "Abstract": "Logical dependencies are implicit relationships established between software artifacts that have evolved together. Software engineering researchers have investigated this kind of dependency to assess fault-proneness, detect design issues, infer code decay, and predict likely changes in code. Despite the acknowledged relation between logical dependencies and software quality, the nature of the logical dependencies is unknown in the literature. Most authors hypothesize about their origins, but no empirical study has been conducted to investigate the real nature of these dependencies. In this paper, we investigated the origins of logical dependencies by means of a case study involving a Java FLOSS project. We mined the project repository, filtered out irrelevant data based on statistical analyses, and performed a manual inspection of the logical dependencies to identify their origins using information from the revision comments, code diffs, and informal interviews held with the developers of the analyzed project. Preliminary results showed that logical dependencies involved files that changed together for a series of different reasons, which ranged from changing software license to refactoring classes that belonged to the same semantic class."
},
{
  "Title": "Using the Gini Coefficient for Bug Prediction in Eclipse",
  "Type": "Workshop Paper",
  "Key": "iwpse51s",
  "Authors": ["Emanuel Giger", "Martin Pinzger", "Harald Gall"],
  "Affiliations": ["University of Zurich", "Delft University of Technology"],
  "Abstract": "The Gini coefficient is a prominent measure to quantify the inequality of a distribution. It is often used in the field of economy to describe how goods, e.g., wealth or farmland, are distributed among people. We use the Gini coefficient to measure code ownership by investigating how changes made to source code are distributed among the developer population. The results of our study with data from the Eclipse platform show that less bugs can be expected if a large share of all changes are accumulated, i.e., carried out, by relatively few developers."
},
{
  "Title": "Requirements Evolution Drives Software Evolution",
  "Type": "Workshop Paper",
  "Key": "iwpse53s",
  "Authors": ["Neil Ernst", "Alexander Borgida", "John Mylopoulos"],
  "Affiliations": ["University of Toronto", "Rutgers University", "University of Trento"],
  "Abstract": "Changes to software should be made with reference to the requirements of that software, as these requirements provide the reasons for a change. Requirements serve to tie the implementation world of the developers to the problem world of the stakeholders. Most empirical studies of requirements have shown that misunderstood and changing requirements cause the majority of failures and costs in software. However, research in software evolution has typically focused on how to evolve software and not why. In our view, evolving software is about solving requirements problems, that is, finding new implementations which will satisfy the requirements while respecting domain assumptions. We argue that by describing this relationship, an implementation choice that best meets stakeholder needs can be made. We describe a tool that models requirements problems. This tool can find incremental solutions to evolving requirements problems quickly."
},
{
  "Title": "Are the Classes that use Exceptions Defect Prone?",
  "Type": "Workshop Paper",
  "Key": "iwpse59s",
  "Authors": ["Cristina Marinescu"],
  "Affiliations": ["Politehnica University of Timisoara"],
  "Abstract": "Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently many empirical studies point out that sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if the design entities (classes) that use exceptions are more defect prone than the other classes. The results, based on analyzing three releases of Eclipse, show that indeed the classes that use exceptions are more defect prone than the other classes. Based on our results, developers are advertised to pay more attention to the way they handle exceptions."
},
{
  "Title": "Challenges of Evolving Sequential to Parallel Code: An Exploratory Review",
  "Type": "Workshop Paper",
  "Key": "iwpse65s",
  "Authors": ["Anne Meade", "Jim Buckley", "J.J. Collins"],
  "Affiliations": ["University of Limerick"],
  "Abstract": "Large legacy systems that have been in use for several decades need to evolve in order to take advantage of new technological advances. One such development is the emergence of multi-core processors and parallel platforms. However, the evolution of code written for single-core platforms into code that can take advantage of multi-core technology is challenging. The aim of this research is to explore the challenges that parallel programmers face in the evolution of existing software to exploit multicore and parallel architectures. A review of the current literature was conducted and ten frequently reported challenges were identified. It is important to raise awareness of potential challenges that practitioners may face when evolving sequential code to exploit multicore platforms in order to be better prepared for future evolution. The research community can use these results to develop a research agenda in order to design and develop solutions to address these challenges."
},
{
  "Title": "Causes of Premature Aging during Software Development: An Observational Study",
  "Type": "Workshop Paper",
  "Key": "iwpse01",
  "Authors": ["Mathieu LavallÃ©e", "Pierre Robillard"],
  "Affiliations": ["Ãcole Polytechnique de MontrÃ©al"],
  "Abstract": "Much work has been done on the subject of what happens to software architecture during maintenance activities. There seems to be a consensus that it degrades during the evolution of the software. More recent work shows that this degradation occurs even during development activities, design decisions are either adjusted or forgotten. Some studies have looked into the causes of this degradation, but these have mostly done so at a very high level. This study examines three projects at code level. Three architectural pre-implementation designs are compared with their post-implementation design counterparts, with special attention paid to the causes of the changes. We found many negative changes causing anti-patterns, at the package, class, and method levels. After analysis of the code, we were able to find the specific reasons for the poor design decisions. Although the underlying causes are varied, they can be grouped into three basic categories. knowledge problems, artifact problems, and management problems. This categorization shows that anti-pattern causes are varied and are not all due to the developers. The main conclusion is that promoting awareness of anti-patterns to developers is insufficient to prevent them since some of the causes escape their grasp."
},
{
  "Title": "Problem-Solution Mapping for Forward and Reengineering on Architectural Level",
  "Type": "Workshop Paper",
  "Key": "iwpse04",
  "Authors": ["Matthias Riebisch", "Stephan Bode", "Robert Brcina"],
  "Affiliations": ["Ilmenau University of Technology"],
  "Abstract": "Software architectures play a key role for the development and evolution of software systems because they have to enable their quality properties such as scalability, flexibility, and security. Software architectural decisions represent a transition from problem space with quality goals and requirements on one side to solution space with technical solutions on the other side. Technical solutions are reusable elements for the work of the architect as for example patterns, styles, frameworks and building blocks. For long-term evolution of the systems, an explicit mapping between goals and solutions is helpful for expressing design knowledge and fundamental decisions. Such a mapping has to bridge between the fields of requirements engineering, software architectural design, and software quality thus enabling reuse. In this paper the Goal Solution Scheme is discussed, which maps quality goals and goal refinements to architectural principles and solutions. The paper extends the approach from the previously discussed forward engineering to re-engineering activities thus covering evolutionary development processes. The evaluation of the approach has been performed in several case studies and projects including a large industrial one."
},
{
  "Title": "Network Analysis of OSS Evolution: An Empirical Study on ArgoUML Project",
  "Type": "Workshop Paper",
  "Key": "iwpse13",
  "Authors": ["Wen Zhang", "Ye Yang", "Qing Wang"],
  "Affiliations": ["Institute of Software, Chinese Academy of Sciences"],
  "Abstract": "While complexity is an essential problem inherent in software system and its development, OSS (Open-Source Software) is not an exception and is not immune to this problem as well. The fast growth of OSS movement has impressed us with reduced cost but high-quality software. To learn some lessons from successful OSS in handling the complexity, social network analysis is prevalent in analyzing both human-aspect and source-code-aspect interaction of OSS. This paper conducted an empirical study of an OSS project-ArgoUML. Unlike most previous studies regarding OSS email archives as a whole social network, our focus is on the quantitative analysis of a series of social networks produced in the process of OSS version evolution and module development. Through the empirical study, we have found that all the social network measures employed in this study are comparable to identify core developers of ArgoUML project. The frequency of co-occurrence of developers within the same topic is not a decisive factor to identify core developers. Developers within the same module communicate closely and frequently with each other. The more modules a developer developed, the more communication he (or she) will have with other developers. Although participants of developers' mailing lists are fluctuating in a large magnitude , the committers of the source code are kept stable in each version evolution. Moreover, the variation of committers of source code in version evolutions is almost unpredictable based on the variation of participants in developers' mailing lists."
},
{
  "Title": "User Generated (Web) Content: Trash or Treasure",
  "Type": "Workshop Paper",
  "Key": "iwpse16",
  "Authors": ["Giovanni Alluvatti", "Andrea Capiluppi", "Giuseppe De Ruvo", "Marco Molfetta"],
  "Affiliations": ["University of Sannio", "University of East London"],
  "Abstract": "It has been claimed that the advent of user-generated content has reshaped the way people approached all sorts of content realization projects, being multimedia (YouTube, DeviantArt, etc.), knowledge (Wikipedia, blogs), to software in general, when based on a more general Open Source model. After many years of research and evidence, several studies have demonstrated that Open Source Software (OSS) portals often contain a large amount of software projects that simply do not evolve, often developed by relatively small communities, and that still struggle to attract a sustained number of contributors. In terms of such content, the ``tragedy'' appears to be that the user demand for content and the offer of experts contributing content are on curves with different slopes, with the demand growing more quickly. In this paper we argue that, even given the differences in the requested expertise, many projects reliant on user-contributed content and expertise undergo a similar evolution, along a logistic growth. a first slow growth rate is followed by a much faster evolution growth. When a project fails to attract more developers i.e. contributors, the evolution of project's content does not present the ``explosive growth'' phase, and it will eventually burnout, and the project appears to be abandoned. Far from being a negative finding, even abandoned project's content provides a valuable resource that could be reused in the future within other projects."
},
{
  "Title": "An Agent-based Framework for Distributed Collaborative Model Evolution",
  "Type": "Workshop Paper",
  "Key": "iwpse19",
  "Authors": ["Hoa Dam", "Aditya Ghose"],
  "Affiliations": ["University of Wollongong"],
  "Abstract": "In recent years, an increasingly large number of software systems have been developed at different geographical regions. As a result, the maintenance and evolution of those systems have shifted from being conducted at a single site to being geographically distributed at multiple locations around the world. In these collaborative development environments, it is a critical challenge to maintain consistency within a software model during its evolution since changes are rapidly and concurrently made to the model without the awareness of team members at various locations. Most of existing software modelling applications however primarily support single-user settings whereas some other recent approaches which rely on version control tools fail to provide effective, real-time support in a collaborative modelling setting requiring frequent interactions and short feedback cycles. In this paper, we present a framework that supports designers in evolving software models in a collaborative modelling setting. This framework is built upon the well-known Belief Desire Intention agent architecture to exploit its robustness and flexibility in maintaining consistency within a design model and resolving conflicts in real time when changes are concurrently made to it by different designers."
},
{
  "Title": "An Editing-operation Replayer with Highlights Supporting Investigation of Program Modifications",
  "Type": "Workshop Paper",
  "Key": "iwpse55td",
  "Authors": ["Takayuki Omori", "Katsuhisa Maruyama"],
  "Affiliations": ["Ritsumeikan University"],
  "Abstract": "In editing source code of a program on modern integrated development environments, automated recording of editing operations has become popular. These operations enable past program modifications to be investigated in detail. However, such investigation of enormous amount of operations is troublesome for a human. Moreover, each of the recorded operations does not indicate what code changes were totally done. To address these problems, this paper proposes OperationReplayer, which replays recorded operations in chronological order and arbitrarily restores past snapshots of source code. It employs a plug-in mechanism that allows its user to flexibly highlight particular operations in their visualization. This mechanism provides the user with various overviews of vast operations and alleviates burden on his/her investigation. The paper also shows three case studies of effective examinations using highlight plug-ins."
},
{
  "Title": "Measuring Multi-language Software Evolution: A Case Study",
  "Type": "Workshop Paper",
  "Key": "iwpse57s",
  "Authors": ["Tom Arbuckle"],
  "Affiliations": ["University of Limerick"],
  "Abstract": "Characterising and measuring software developed in multiple languages is a problem for practitioners. Rather than a language-based approach, we avoid difficulties related to syntax, semantics and language paradigms by looking directly at relative shared information content to perform these tasks. Measuring, for each language, the relative number of bits of shared binary information between artefacts representative of consecutive releases of the project using a common tool permits the direct comparison of evolution results for the multiple languages. This paper presents a case study of the program suite called git, written in C, perl and Bourne shell. The study uses this method to show that, for git, code in scripting languages does not prototype later C, Bourne shell and C code are written together and that the languages code contributions occur concurrently."
},
{
  "Title": "Challenges in Model-Based Evolution and Merging of Access Control Policies",
  "Type": "Workshop Paper",
  "Key": "iwpse61s",
  "Authors": ["Lionel Montrieux", "Michel Wermelinger", "Yijun Yu"],
  "Affiliations": ["The Open University"],
  "Abstract": "Access Control plays a crucial part in software security, as it is responsible for making sure that users have access to the resources they need while being forbidden from accessing resources they do not need. Access control models such as Role-Based Access Control have been developed to help system administrators deal with the increasing complexity of the rules that determine whether or not a particular user should access a particular resource. These rules, as well as the users and their needs, are likely to evolve over time. In some cases, it may even be necessary to merge several access control configurations into a single one. In this position paper, we review existing research in model-based software evolution and merging, and argue the need for a specific approach for access control in order to take its specific requirements into account."
},
{
  "Title": "Historage: Fine-grained Version Control System for Java",
  "Type": "Workshop Paper",
  "Key": "iwpse63td",
  "Authors": ["Hideaki Hata", "Osamu Mizuno", "Tohru Kikuno"],
  "Affiliations": ["Osaka University", "Kyoto Institute of Technology"],
  "Abstract": "Software systems are changed continuously for adapting to the environment, correcting faults, improving performance, and so on. For in-depth analysis related to software evolution, it is informative to obtain the histories of fine-grained source code entities. This paper presents a tool named Historage that can provide entire histories of fine-grained entities in Java, such as methods, constructors, fields, etc. A characteristic of Historage is the ability of tracing entity histories including renaming changes. We applied our technique to five open source software projects to quantitatively evaluate the renaming change identification."
},
{
  "Title": "Labeling Library Functions in Stripped Binaries",
  "Type": "Workshop Paper",
  "Key": "paste04",
  "Authors": ["Emily Jacobson", "Nathan Rosenblum", "Barton Miller"],
  "Affiliations": ["University of Wisconsin"],
  "Abstract": "Binary code presents unique analysis challenges, particularly when debugging information has been stripped from the executable. Among the valuable information lost in stripping are the identities of standard library functions linked into the executable; knowing the identities of such functions can help to optimize automated analysis and is instrumental in understanding program behavior. Library fingerprinting attempts to restore the names of library functions in stripped binaries, using signatures extracted from reference libraries. Existing methods are brittle in the face of variations in the toolchain that produced the reference libraries and do not generalize well to new library versions. We introduce semantic descriptors, high-level representations of library functions that avoid the brittleness of existing approaches. We have extended a tool, unstrip, to apply this technique to fingerprint wrapper functions in the GNU C library. unstrip discovers functions in a stripped binary and outputs a new binary, with meaningful names added to the symbol table. Other tools can leverage these symbols to perform further analysis. We demonstrate that our semantic descriptors generalize well and substantially outperform existing library fingerprinting techniques."
},
{
  "Title": "An Evaluation of Change-Based Coverage Criteria",
  "Type": "Workshop Paper",
  "Key": "paste07",
  "Authors": ["Marc Fisher II", "Jan Wloka", "Frank Tip", "Barbara Ryder", "Alexander Luchansky"],
  "Affiliations": ["Virginia Tech", "IBM Rational Research Lab", "IBM Research", "Vanguard Group, Inc."],
  "Abstract": "Various coverage criteria are commonly used to assess the quality of test suites, but achieving full coverage according to these criteria is often impossible or impractical. Our research starts from the popular assumption that a disproportionate number of faults is likely to reside in recently changed code. Based on this assumption, we propose several change-based coverage criteria that reflect to what extent changes with respect to a previous program version are exercised by a test suite. In a set of experiments on programs from the SIR repository, we found change-based criteria to reveal faults bet- ter than traditional criteria, and to enable the construction of much smaller test suites with similar fault detection effectiveness. We also report on a case study that shows that achieving 100% coverage according to a change-based criterion is feasible and that by doing so we were able to find additional faults, including one fault that was not intentionally seeded in the subject program."
},
{
  "Title": "Anywhere, Any-Time Binary Instrumentation",
  "Type": "Workshop Paper",
  "Key": "paste08",
  "Authors": ["Andrew Bernat", "Barton Miller"],
  "Affiliations": ["University of Wisconsin at Madison"],
  "Abstract": "The Dyninst binary instrumentation and analysis framework distinguishes itself from other binary instrumentation tools through its abstract, machine independent interface; its emphasis on anywhere, any-time binary instrumentation; and its low overhead that is proportional to the number of instrumented locations. Dyninst represents the program in terms of familiar control flow structures such as functions, loops, and basic blocks, and users manipulate these representations to insert instrumentation anywhere in the binary. We use graph transformation techniques to insure that this instrumentation executes when desired even when instrumenting highly optimized (or malicious) code that other instrumenters cannot correctly instrument. Unlike other binary instrumenters, Dyninst can instrument at any time in the execution continuum, from static instrumentation (binary rewriting) to instrumenting actively executing code (dynamic instrumentation). Furthermore, we allow users to modify or remove instrumentation at any time, with such modifications taking immediate effect. Our analysis techniques allow us to insert new code without modifying uninstrumented code; as a result, all uninstrumented code executes at native speed. We demonstrate that our techniques provide this collection of capabilities while imposing similar or lower overhead than other widely used instrumenters."
},
{
  "Title": "Towards Systematic, Comprehensive Trace Generation for Behavioral Pattern Detection through Symbolic Execution",
  "Type": "Workshop Paper",
  "Key": "paste09s",
  "Authors": ["Markus von Detten"],
  "Affiliations": ["University of Paderborn"],
  "Abstract": "In reverse engineering, dynamic pattern detection is accomplished by collecting execution traces and comparing them to expected behavioral patterns. The traces are collected by manually executing the program under study and therefore represent only part of all relevant program behavior. This can lead to false conclusions about the detected patterns. In this paper, we propose to generate all relevant program traces by using symbolic execution. In order to reduce the created trace data, we allow to limit the trace collection to a user-selectable subset of the statically detected pattern candidates."
},
{
  "Title": "Locating Failure-Inducing Environment Changes",
  "Type": "Workshop Paper",
  "Key": "paste10",
  "Authors": ["Dawei Qi", "Minh Ngo", "Tao Sun", "Abhik Roychoudhury"],
  "Affiliations": ["National University of Singapore"],
  "Abstract": "Traditionally, debugging refers to the process of locating the program portions which are responsible for a program failure. However, a program also fails when the execution environment does not meet the requirement-assumption of the program. Unfortunately, few existing debugging techniques addresses the problem of changing operating system environment. In this paper, we propose an effective record-replay technique called Semi-replay to solve this problem. Semi-replay records all the essential interactions between an application and its underlying operating system environment where it successfully executed. Semi-replay then allows the recorded interactions to be partially replayed and partially executed in another operating system to identify those interactions which contribute to the root cause of the application failure induced by the environment changes. We have conducted three case studies on real-life programs which show the significance and efficiency of the Semi-replay technique in locating failure-inducing environment changes. We have also implemented a tool for the Linux kernel to demonstrate the feasibility of the proposed approach."
},
{
  "Title": "Assessing Modularity via Usage Changes",
  "Type": "Workshop Paper",
  "Key": "paste13s",
  "Authors": ["Yana Mileva", "Andreas Zeller"],
  "Affiliations": ["Saarland University"],
  "Abstract": "Good program design strives towards modularity, that is, limiting the effects of changes to the code. We assess the modularity of software modules by mining change histories, The more a change to a module implementation changes its usage in client code, the lower its modularity. In an early analysis of four different releases of open-source projects, we found that changes can differ greatly in their impact on client code, and that such impact helps in assessing modularity."
},
{
  "Title": "Program Synthesis for Automating End-user Programming and Education",
  "Type": "Keynote",
  "Key": "ProgramSynthesisforAutomatingEnduserProgrammingandEducation",
  "Authors": ["Sumit Gulwani"],
  "Affiliations": ["Microsoft Research"],
  "Abstract": "Recent research in program synthesis has made it possible to effectively synthesize small programs in a variety of domains. In this talk, I will describe two useful applications of this technology that have the potential to influence daily lives of billions of people. One application involves automating end-user programming using examples, which can allow non-programmers to effectively use computational devices such as computers, cell-phones (and in the future robots) to perform a variety of repetitive tasks. Another application involves building automated tutoring systems that can help students with problem solving in math and science domains."
},
{
  "Title": "On the Interplay between Software Architects and Software Engineers in an Agile Environment: Who Should Do What?",
  "Type": "Workshop Paper",
  "Key": "OntheInterplaybetweenSoftwareArchitectsandSoftwareEngineersinanAgileEnvironmentWhoShouldDoWhat",
  "Authors": ["Antony Tang", "Ton Gerrits", "Peter Nacken", "Hans Vliet"]
},
{
  "Title": "The Learning Component in Social Software Engineering",
  "Type": "Workshop Paper",
  "Key": "TheLearningComponentinSocialSoftwareEngineering",
  "Authors": ["Pierre Robillard"]
},
{
  "Title": "Extending Socio-technical Congruence with Awareness Relationships",
  "Type": "Workshop Paper",
  "Key": "ExtendingSociotechnicalCongruencewithAwarenessRelationships",
  "Authors": ["Irwin Kwan", "Daniela Damian"]
},
{
  "Title": "Augmenting Social Awareness in a Collaborative Development Environment",
  "Type": "Workshop Paper",
  "Key": "AugmentingSocialAwarenessinaCollaborativeDevelopmentEnvironment",
  "Authors": ["Fabio Calefato", "Filippo Lanubile", "Nicola Sanitate", "Giuseppe Santoro"]
},
{
  "Title": "Towards Systematic Analysis of Continuous User Input",
  "Type": "Workshop Paper",
  "Key": "TowardsSystematicAnalysisofContinuousUserInput",
  "Authors": ["Dennis Pagano"]
},
{
  "Title": "Secret Ninja Testing with HALO Software Engineering",
  "Type": "Workshop Paper",
  "Key": "SecretNinjaTestingwithHALOSoftwareEngineering",
  "Authors": ["Jonathan Bell", "Swapneel Sheth", "Gail Kaiser"]
},
{
  "Title": "Socially Mediated Technology Awareness",
  "Type": "Workshop Paper",
  "Key": "SociallyMediatedTechnologyAwareness",
  "Authors": ["Thomas Fritz", "Gail Murphy"]
},
{
  "Title": "A Dashboard for Community Cultivation within Social Requirements Engineering",
  "Type": "Workshop Paper",
  "Key": "ADashboardforCommunityCultivationwithinSocialRequirementsEngineering",
  "Authors": ["Anna Hannemann", "Yan Wang"]
},
{
  "Title": "Engineering Software Engineering Teams",
  "Type": "Workshop Paper",
  "Key": "EngineeringSoftwareEngineeringTeams",
  "Authors": ["Patrick Wagstrom"]
},
{
  "Title": "Online Social Networks as a Catalyst for Software and IT Innovation",
  "Type": "Workshop Paper",
  "Key": "OnlineSocialNetworksasaCatalystforSoftwareandITInnovation",
  "Authors": ["Leif Singer", "Norbert Seyff", "Samuel Fricker"]
}
	],
	"Sessions": [
{
  "Title": "Invited talk",
  "Type": "Invited Talk",
  "ShortType": "Invited Talk",
  "Key": "dst1",
  "Day": "9/5/2011",
  "Time": "9:00 am - 9:30 am",
  "Location": "Seminar room",
  "Items": ["Howtoperformareliablesoftwareengineeringempiricalstudy"]
},
{
  "Title": "Development documentation",
  "ShortTitle": "DS1",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "ds1",
  "Day": "9/5/2011",
  "Time": "9:30 am - 10:30 am",
  "Location": "Seminar room",
  "Items": ["fse03d", "fse06d", "fse08d"],
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak",
  "Day": "9/5/2011",
  "Time": "10:30 am - 11:00 am",
  "Location": "Lobby"
},
{
  "Title": "Specification Mining",
  "ShortTitle": "DS2",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "ds2",
  "Day": "9/5/2011",
  "Time": "11:00 am - 11:40 am",
  "Location": "Seminar room",
  "Items": ["fse01d", "fse11d"],
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Closed discussion of morning presentations",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "Closeddiscussionofmorningpresentations",
  "Day": "9/5/2011",
  "Time": "11:40 am - 12:30 pm",
  "Location": "Seminar room"
},
{
  "Title": "Lunch",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "Lunch",
  "Day": "9/5/2011",
  "Time": "12:30 pm - 2:00 pm",
  "Location": "Atrium",
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Invited talk",
  "Type": "Invited Talk",
  "ShortType": "Invited Talk",
  "Key": "dst2",
  "Day": "9/5/2011",
  "Time": "2:00 pm - 2:30 pm",
  "Location": "Seminar room",
  "Items": ["Howtowriteanexcellentsoftwareengineeringpaper"]
},
{
  "Title": "Testing",
  "ShortTitle": "DS3",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "ds3",
  "Day": "9/5/2011",
  "Time": "2:30 pm - 3:30 pm",
  "Location": "Seminar room",
  "Items": ["fse04d", "fse05d", "fse10d"],
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-1",
  "Day": "9/5/2011",
  "Time": "3:30 pm - 4:00 pm",
  "Location": "Lobby"
},
{
  "Title": "Adaptation",
  "ShortTitle": "DS4",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "ds4",
  "Day": "9/5/2011",
  "Time": "4:00 pm - 4:40 pm",
  "Location": "Seminar room",
  "Items": ["fse07d", "fse09d"],
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Closed discussion of afternoon presentations",
  "Type": "Doctoral Symposium",
  "ShortType": "Doctoral Symposium",
  "Key": "Closeddiscussionofafternoonpresentations",
  "Day": "9/5/2011",
  "Time": "4:40 pm - 5:30 pm",
  "Location": "Seminar room",
  "Chairs": ["Mark Harman", "Antonia Bertolino"]
},
{
  "Title": "Opening session",
  "Type": "Plenary",
  "ShortType": "Plenary",
  "Key": "Openingsession",
  "Day": "9/7/2011",
  "Time": "9:00 am - 9:30 am",
  "Location": "Congress hall"
},
{
  "Title": "Keynote",
  "Type": "Keynote",
  "ShortType": "Keynote",
  "Key": "kn1",
  "Day": "9/7/2011",
  "Time": "9:30 am - 10:30 am",
  "Location": "Congress hall",
  "Items": ["fse21ws"]
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-2",
  "Day": "9/7/2011",
  "Time": "10:30 am - 11:00 am",
  "Location": "Lobby"
},
{
  "Title": "Bugs and Changes",
  "ShortTitle": "RT1",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt1",
  "Day": "9/7/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Congress hall",
  "Items": ["esec119", "esec154", "esec159"]
},
{
  "Title": "Models and Requirements",
  "ShortTitle": "RT2",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt2",
  "Day": "9/7/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec080", "esec088", "esec108"]
},
{
  "Title": "Keynote / Security",
  "ShortTitle": "IT1",
  "Type": "Industrial Track",
  "ShortType": "Industrial Track",
  "Key": "it1",
  "Day": "9/7/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Lecture room 1",
  "Items": ["HybridAnalysisforJavaScriptSecurityAssessment", "ProductivityinITservicesKeynote"],
  "Chairs": ["Frank Tip", "Volker Gruhn"]
},
{
  "Title": "Lunch",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "Lunch-1",
  "Day": "9/7/2011",
  "Time": "12:30 pm - 2:00 pm",
  "Location": "Atrium"
},
{
  "Title": "Empirical Studies",
  "ShortTitle": "RT3",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt3",
  "Day": "9/7/2011",
  "Time": "2:00 pm - 3:00 pm",
  "Location": "Congress hall",
  "Items": ["esec120", "esec126", "esec138"]
},
{
  "Title": "Analysis I",
  "ShortTitle": "RT4",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt4",
  "Day": "9/7/2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec049", "esec076", "esec167"]
},
{
  "Title": "Testing",
  "ShortTitle": "IT2",
  "Type": "Industrial Track",
  "ShortType": "Industrial Track",
  "Key": "it2",
  "Day": "9/7/2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Lecture room 1",
  "Items": ["ConcolicTestingonEmbeddedSoftwareCaseStudiesonMobilePlatformPrograms", "FasterFaultFindingatGoogleusingMultiObjectiveRegressionTestOptimisation", "ManagingPerformanceTestingWithReleaseCertificationandDataCorrelation"],
  "Chairs": ["Frank Tip", "Volker Gruhn"]
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-3",
  "Day": "9/7/2011",
  "Time": "3:30 pm - 4:00 pm",
  "Location": "Lobby"
},
{
  "Title": "Debugging",
  "ShortTitle": "RT5",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt5",
  "Day": "9/7/2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Congress hall",
  "Items": ["esec015", "esec141", "esec176"]
},
{
  "Title": "Collaboration",
  "ShortTitle": "RT6",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt6",
  "Day": "9/7/2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec006", "esec007", "esec195"]
},
{
  "Title": "Modern Software Development",
  "ShortTitle": "IT3",
  "Type": "Industrial Track",
  "ShortType": "Industrial Track",
  "Key": "it3",
  "Day": "9/7/2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Lecture room 1",
  "Items": ["ATrueStoryofRefactoringaLargeOraclePLSQLBankingSystem", "AutomotiveSystemDevelopmentBasedonCollaborativeModelingUsingMultipleADLs", "DoesPairProgrammingIncreaseDevelopersAttention"],
  "Chairs": ["Frank Tip", "Volker Gruhn"]
},
{
  "Title": "Organ Concert",
  "Type": "Social",
  "ShortType": "Social",
  "Key": "OrganConcert",
  "Day": "9/7/2011",
  "Time": "7:00 pm - 7:30 pm",
  "Location": "Votive Church"
},
{
  "Title": "Welcome Reception",
  "Type": "Social",
  "ShortType": "Social",
  "Key": "WelcomeReception",
  "Day": "9/7/2011"
},
{
  "Title": "Keynote",
  "Type": "Keynote",
  "ShortType": "Keynote",
  "Key": "kn2",
  "Day": "9/8/2011",
  "Time": "9:00 am - 10:00 am",
  "Location": "Congress hall",
  "Items": ["fse23gz"]
},
{
  "Title": "Posters",
  "Type": "Exhibition",
  "ShortType": "Exhibition",
  "Key": "Posters",
  "Day": "9/8/2011",
  "Time": "10:00 am - 10:30 am",
  "Location": "Exhibition area"
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-4",
  "Day": "9/8/2011",
  "Time": "10:30 am - 11:00 am",
  "Location": "Lobby"
},
{
  "Title": "Testing",
  "ShortTitle": "RT7",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt7",
  "Day": "9/8/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Congress hall",
  "Items": ["esec039", "esec090", "esec194"]
},
{
  "Title": "New Ideas I",
  "ShortTitle": "NI1",
  "Type": "New Ideas",
  "ShortType": "New Ideas",
  "Key": "ni1",
  "Day": "9/8/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Lecture hall",
  "Items": ["new03a", "new04z", "new07p", "new08a", "new10b", "IntroductiontotheNewIdeasTrack"],
  "Chair": "Martin Robillard"
},
{
  "Title": "Lunch",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "Lunch-2",
  "Day": "9/8/2011",
  "Time": "12:30 pm - 2:00 pm",
  "Location": "Atrium"
},
{
  "Title": "Tool Demonstrations I",
  "ShortTitle": "TD1",
  "Type": "Tool Demonstrations",
  "ShortType": "Tool Demonstrations",
  "Key": "td1",
  "Day": "9/8/2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Congress hall",
  "Items": ["esec301", "esec305", "esec306", "esec308", "esec310", "esec313", "esec314"],
  "Chairs": ["Michele Lanza", "Anthony Cleve"]
},
{
  "Title": "New Ideas II",
  "ShortTitle": "NI2",
  "Type": "New Ideas",
  "ShortType": "New Ideas",
  "Key": "ni2",
  "Day": "9/8/2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Lecture hall",
  "Items": ["new01e", "new02c", "new05c", "new06k", "new09i", "new11a"],
  "Chair": "Martin Robillard"
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-5",
  "Day": "9/8/2011",
  "Time": "3:30 pm - 4:00 pm",
  "Location": "Lobby"
},
{
  "Title": "Tool Demonstrations II",
  "ShortTitle": "TD2",
  "Type": "Tool Demonstrations",
  "ShortType": "Tool Demonstrations",
  "Key": "td2",
  "Day": "9/8/2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Congress hall",
  "Items": ["esec302", "esec303", "esec304", "esec307", "esec309", "esec311", "esec312"],
  "Chairs": ["Michele Lanza", "Anthony Cleve"]
},
{
  "Title": "Configurations",
  "ShortTitle": "RT8",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt8",
  "Day": "9/8/2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec003", "esec034", "esec121"]
},
{
  "Title": "ACM SIGSOFT Outstanding Research Award",
  "Type": "Keynote",
  "ShortType": "Keynote",
  "Key": "kn3",
  "Day": "9/9/2011",
  "Time": "9:00 am - 10:30 am",
  "Location": "Congress hall",
  "Items": ["fse25dg"]
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-6",
  "Day": "9/9/2011",
  "Time": "10:30 am - 11:00 am",
  "Location": "Lobby"
},
{
  "Title": "Analysis II",
  "ShortTitle": "RT9",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt9",
  "Day": "9/9/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Congress hall",
  "Items": ["esec071", "esec096", "esec198"]
},
{
  "Title": "Defects",
  "ShortTitle": "RT10",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt10",
  "Day": "9/9/2011",
  "Time": "11:00 am - 12:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec098", "esec160", "esec181"]
},
{
  "Title": "Lunch",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "Lunch-3",
  "Day": "9/9/2011",
  "Time": "12:30 pm - 2:00 pm",
  "Location": "Atrium"
},
{
  "Title": "ACM SIGSOFT Impact Paper Award",
  "Type": "Keynote",
  "ShortType": "Keynote",
  "Key": "kn4",
  "Day": "9/9/2011",
  "Time": "2:00 pm - 3:00 pm",
  "Location": "Congress hall"
},
{
  "Title": "Informal Tool Demonstrations",
  "Type": "Exhibition",
  "ShortType": "Exhibition",
  "Key": "InformalToolDemonstrations",
  "Day": "9/9/2011",
  "Time": "3:00 pm - 3:30 pm",
  "Location": "Exhibition area"
},
{
  "Title": "PhD Working Groups - demonstrations",
  "Type": "Exhibition",
  "ShortType": "Exhibition",
  "Key": "PhDWorkingGroupsdemonstrations",
  "Day": "9/9/2011",
  "Time": "3:00 pm - 3:30 pm",
  "Location": "Exhibition area",
  "Chair": "ÃrpÃ¡d BeszÃ©des"
},
{
  "Title": "Coffee Break",
  "Type": "Break",
  "ShortType": "Break",
  "Key": "CoffeeBreak-7",
  "Day": "9/9/2011",
  "Time": "3:30 pm - 4:00 pm",
  "Location": "Lobby"
},
{
  "Title": "Informal Tool Demonstrations",
  "Type": "Exhibition",
  "ShortType": "Exhibition",
  "Key": "InformalToolDemonstrations-1",
  "Day": "9/9/2011",
  "Time": "4:00 pm - 4:30 pm",
  "Location": "Exhibition area"
},
{
  "Title": "PhD Working Groups - presentations",
  "Type": "Exhibition",
  "ShortType": "Exhibition",
  "Key": "PhDWorkingGroupspresentations",
  "Day": "9/9/2011",
  "Time": "4:00 pm - 4:30 pm",
  "Location": "Congress hall",
  "Chair": "ÃrpÃ¡d BeszÃ©des"
},
{
  "Title": "Analysis III",
  "ShortTitle": "RT11",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt11",
  "Day": "9/9/2011",
  "Time": "4:30 pm - 5:30 pm",
  "Location": "Congress hall",
  "Items": ["esec053", "esec117"]
},
{
  "Title": "Mining",
  "ShortTitle": "RT12",
  "Type": "Research Track",
  "ShortType": "Research Track",
  "Key": "rt12",
  "Day": "9/9/2011",
  "Time": "4:30 pm - 5:30 pm",
  "Location": "Lecture hall",
  "Items": ["esec041", "esec132"]
},
{
  "Title": "Closing session",
  "Type": "Plenary",
  "ShortType": "Plenary",
  "Key": "Closingsession",
  "Day": "9/9/2011",
  "Time": "5:30 pm - 6:00 pm",
  "Location": "Congress hall"
},
{
  "Title": "8th International Workshop on Software Quality",
  "ShortTitle": "WOSQ",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws1",
  "Day": "9/4/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "Software becomes ever more feature-rich and thereby harder to distinguish based on its functionality. Quality now differentiates between similar software products. Specifying, constructing, and assuring quality has been under research for several decades and continues to be a long-term research area, because of its many facets and its complexity. Current national and international initiatives show that there is an active research community in academia and industry. The series of workshops on software quality are a forum to discuss and advance the state-of-the-art research and practice in software quality. This year's edition of the workshop is co-located with the joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering and will bring some changes in the workshop format. We still have a keynote and paper presentations, but there is also a discussion session to bring up new topics and for the attendees to share their experiences. We will have a keynote by Motoei AZUMA, emeritus professor at Waseda University, Tokyo, Japan, about the advances of the SQUaRE project, which works on the new software product quality standards series ISO 25000. ",
  "Workshop": true,
  "Items": ["wosq01s", "wosq02k", "wosq04m", "wosq05y", "wosq07a", "wosq09s", "wosq10g", "wosq11ma"],
  "Chairs": ["Stefan Wagner", "Sunita Chulani", "Bernard Wong"]
},
{
  "Title": "Workshop on Assurances for Self-Adaptive Systems (ASAS 2011)",
  "ShortTitle": "ASAS",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws2",
  "Day": "9/4/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "Assurances for Self-Adaptive Systems (ASAS) is a workshop that will bring together researchers to discuss software engineering aspects of self-adaptive systems, including methods, architectures, languages, algorithms, techniques, and tools that can be used to support assurances in self-adaptive system development. ASAS is intended as a complement to the efforts started a while ago at the successful FSE workshop series on Self-Healing Systems (WOSS), or the Software Engineering for Adaptive and Self-Managing Systems (SEAMS) symposium. However, in contrast with those events, ASAS is focused on the collection, storage, and analysis of evidence for the provision of assurances that a self-adaptive software system is able to behave functionally and non-functionally according to its specification. ",
  "Workshop": true,
  "Items": ["asas01rc", "asas04am", "asas07c", "asas10b", "asas13d", "asas16sp", "RVARuntimeVerificationFrameworkforMonitoringPredictionandMining"],
  "Chairs": ["Javier CÃ¡mara", "RogÃ©rio de Lemos", "Carlo Ghezzi", "AntÃ³nia Lopes"]
},
{
  "Title": "12th International Workshop on Principles on Software Evolution - 7th ERCIM Workshop on Software Evolution Day 1",
  "ShortTitle": "IWPSE-Evol Day 1",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws3a",
  "Day": "9/5/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "Research in software evolution and evolvability has been thriving in the past years, with a constant stream of new formalisms, tools, techniques, and development methodologies trying, on the one hand, to facilitate the way long-lived successful software systems can be changed in order to cope with demands from users and the increasing complexity and volatility of the contexts in which such systems operate, and, on the other hand, to understand and if possible control the processes by which demand for these changes come about. This workshop is the merger of the annual ERCIM Workshop on Software Evolution (EVOL) and the International Workshop on Principles of Software Evolution (IWPSE). The rationale for a common event is to capitalize on the synergies to be found when theorists and practitioners meet. In 2011, IWPSE-EVOL will be co-located with ESEC/FSE, in early september in Szeged, Hungary. Proceedings of the workshop will be published in the ACM digital library; a special issue of the best papers will be published in the Journal of Software Maintenance and Evolution, Research and Practice. IWPSE-EVOL will feature a special event dedicated to the memory of Prof. Manny Lehman, known as the Father of Software Evolution, who recently passed away on Wednesday, 29th of december, 2010. ",
  "Workshop": true,
  "Items": ["iwpse07", "iwpse10", "iwpse22", "iwpse25", "iwpse51s", "iwpse53s", "iwpse59s", "iwpse65s"],
  "Chairs": ["Romain Robbes", "Anthony Cleve"]
},
{
  "Title": "12th International Workshop on Principles on Software Evolution - 7th ERCIM Workshop on Software Evolution Day 2",
  "ShortTitle": "IWPSE-Evol Day 2",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws3b",
  "Day": "9/6/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "Research in software evolution and evolvability has been thriving in the past years, with a constant stream of new formalisms, tools, techniques, and development methodologies trying, on the one hand, to facilitate the way long-lived successful software systems can be changed in order to cope with demands from users and the increasing complexity and volatility of the contexts in which such systems operate, and, on the other hand, to understand and if possible control the processes by which demand for these changes come about. This workshop is the merger of the annual ERCIM Workshop on Software Evolution (EVOL) and the International Workshop on Principles of Software Evolution (IWPSE). The rationale for a common event is to capitalize on the synergies to be found when theorists and practitioners meet. In 2011, IWPSE-EVOL will be co-located with ESEC FSE, in early september in Szeged, Hungary. Proceedings of the workshop will be published in the ACM digital library; a special issue of the best papers will be published in the Journal of Software Maintenance and Evolution, Research and Practice. IWPSE-EVOL will feature a special event dedicated to the memory of Prof. Manny Lehman, known as the Father of Software Evolution, who recently passed away on Wednesday, 29th of december, 2010. ",
  "Workshop": true,
  "Items": ["iwpse01", "iwpse04", "iwpse13", "iwpse16", "iwpse19", "iwpse55td", "iwpse57s", "iwpse61s", "iwpse63td"],
  "Chairs": ["Romain Robbes", "Anthony Cleve"]
},
{
  "Title": "10th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering",
  "ShortTitle": "PASTE",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws4",
  "Day": "9/5/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "PASTE 2011 is the tenth workshop in a series that meets roughly every 18 months, alternating between programming language and software engineering venues and intending to serve as a bridge between the two. The meeting brings together the program analysis, software tools, and software engineering communities to focus on applications of program analysis techniques in software tools. ",
  "Workshop": true,
  "Items": ["paste04", "paste07", "paste08", "paste09s", "paste10", "paste13s", "ProgramSynthesisforAutomatingEnduserProgrammingandEducation"],
  "Chairs": ["Jeff Foster", "Lori Pollock"]
},
{
  "Title": "The 4th International Workshop on Social Software Engineering",
  "ShortTitle": "SSE",
  "Type": "Co-Located Workshop",
  "ShortType": "Co-Located Workshop",
  "Key": "ws5",
  "Day": "9/5/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "Software is created by people and for people. People are heterogeneous in their beliefs, backgrounds, and preferences. Accommodating and exploiting the social variety is crucial for successful engineering and usage of software. On the one hand, software engineering is a social activity, performed by different individuals and teams. This necessitates methodologies and tools to deal with issues such as communication, coordination, knowledge sharing, compensation, and reconciliation. On the other hand, Social Software (Internet Forums, Wikis, Social Networks, Blogs, etc.) is an expanding computing paradigm, which inherently incorporates intensive social interactions and implications. Engineering Social Software magnifies a spectrum of challenges like group requirements engineering, social-awareness, privacy, security, and trust. \n\n\n\n\n\nBoth directions -- engineering Social Software and treating software engineering as a social activity -- require competency from other disciplines as diverse as psychology, sociology, and organizational science. While both directions receive considerable attention, research in both fields is fragmented, uncoordinated, and partially redundant. The goal of this workshop is to confluence the research on social aspects in software engineering and engineering of Social software into a new field of Social Software Engineering (SSE). ",
  "Workshop": true,
  "Items": ["OntheInterplaybetweenSoftwareArchitectsandSoftwareEngineersinanAgileEnvironmentWhoShouldDoWhat", "TheLearningComponentinSocialSoftwareEngineering", "ExtendingSociotechnicalCongruencewithAwarenessRelationships", "AugmentingSocialAwarenessinaCollaborativeDevelopmentEnvironment", "TowardsSystematicAnalysisofContinuousUserInput", "SecretNinjaTestingwithHALOSoftwareEngineering", "SociallyMediatedTechnologyAwareness", "ADashboardforCommunityCultivationwithinSocialRequirementsEngineering", "EngineeringSoftwareEngineeringTeams", "OnlineSocialNetworksasaCatalystforSoftwareandITInnovation"],
  "Chairs": ["Walid Maalej", "Raian Ali"]
},
{
  "Title": "The 3rd International Symposium on Search Based Software Engineering Day 1",
  "ShortTitle": "SSBSE Day 1",
  "Type": "Co-Located Symposium",
  "ShortType": "Co-Located Symposium",
  "Key": "ws6a",
  "Day": "9/10/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "The objective of the Symposium on Search Based Software Engineering (SSBSE) is to bring the rapidly growing international search based software engineering community together in a welcoming forum for discussion and dissemination, as well as to sustain the flourishing of interest in the field from researchers and practitioners in software engineering and metaheuristic search alike. SSBSE 2011 intends to further participation from the wider software engineering research community through co-location with the internationally renowned joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering - ESEC FSE 2011 - to be held in Szeged, Hungary. SSBSE 2011 is the third international symposium on search based software engineering, and will further build on the success of the inaugural symposium, SSBSE 2009, held in Windsor, UK; and the second symposium, SSBSE 2010, held in Benevento, Italy. ",
  "Workshop": true,
  "Chairs": ["Phil McMinn", "Myra Cohen", "Mel Ã CinnÃ©ide", "Westley Weimer"]
},
{
  "Title": "The 3rd International Symposium on Search Based Software Engineering Day 2",
  "ShortTitle": "SSBSE Day 2",
  "Type": "Co-Located Symposium",
  "ShortType": "Co-Located Symposium",
  "Key": "ws6b",
  "Day": "9/11/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "The objective of the Symposium on Search Based Software Engineering (SSBSE) is to bring the rapidly growing international search based software engineering community together in a welcoming forum for discussion and dissemination, as well as to sustain the flourishing of interest in the field from researchers and practitioners in software engineering and metaheuristic search alike. SSBSE 2011 intends to further participation from the wider software engineering research community through co-location with the internationally renowned joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering - ESEC FSE 2011 - to be held in Szeged, Hungary. SSBSE 2011 is the third international symposium on search based software engineering, and will further build on the success of the inaugural symposium, SSBSE 2009, held in Windsor, UK; and the second symposium, SSBSE 2010, held in Benevento, Italy. ",
  "Workshop": true,
  "Chairs": ["Phil McMinn", "Myra Cohen", "Mel Ã CinnÃ©ide", "Westley Weimer"]
},
{
  "Title": "The 3rd International Symposium on Search Based Software Engineering Day 3",
  "ShortTitle": "SSBSE Day 3",
  "Type": "Co-Located Symposium",
  "ShortType": "Co-Located Symposium",
  "Key": "ws6c",
  "Day": "9/12/2011",
  "Time": "9:00 am - 5:30 pm",
  "Location": "University Congress Centre",
  "Abstract": "The objective of the Symposium on Search Based Software Engineering (SSBSE) is to bring the rapidly growing international search based software engineering community together in a welcoming forum for discussion and dissemination, as well as to sustain the flourishing of interest in the field from researchers and practitioners in software engineering and metaheuristic search alike. SSBSE 2011 intends to further participation from the wider software engineering research community through co-location with the internationally renowned joint meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering - ESEC FSE 2011 - to be held in Szeged, Hungary. SSBSE 2011 is the third international symposium on search based software engineering, and will further build on the success of the inaugural symposium, SSBSE 2009, held in Windsor, UK; and the second symposium, SSBSE 2010, held in Benevento, Italy. ",
  "Workshop": true,
  "Chairs": ["Phil McMinn", "Myra Cohen", "Mel Ã CinnÃ©ide", "Westley Weimer"]
}
	],
	"People": [
{
    "Name": "Abhik Roychoudhury",
    "Affiliation": "National University of Singapore"
},
{
    "Name": "Abram Hindle",
    "Affiliation": "University of California, Davis"
},
{
    "Name": "Aditya Ghose",
    "Affiliation": "University of Wollongong"
},
{
    "Name": "Ahmed Hassan",
    "Affiliation": "Queen's University"
},
{
    "Name": "Ahmed Tamrawi",
    "Affiliation": "Iowa State University"
},
{
    "Name": "Alberto Sillitti"
},
{
    "Name": "Alex Aiken",
    "Affiliation": "Stanford University"
},
{
    "Name": "Alexander Borgida",
    "Affiliation": "Rutgers University"
},
{
    "Name": "Alexander Egyed",
    "Affiliation": "Johannes Kepler University"
},
{
    "Name": "Alexander Luchansky",
    "Affiliation": "Vanguard Group, Inc."
},
{
    "Name": "Allan Gregersen",
    "Affiliation": "University of Southern Denmark"
},
{
    "Name": "Alvin Cheung",
    "Affiliation": "MIT CSAIL"
},
{
    "Name": "Amanda Burton",
    "Affiliation": "University of Wisconsin"
},
{
    "Name": "Amiram Yehudai",
    "Affiliation": "Tel Aviv University"
},
{
    "Name": "Anders MÃ¸ller",
    "Affiliation": "Aarhus University"
},
{
    "Name": "Andrea Arcuri",
    "Affiliation": "Simula Research Laboratory"
},
{
    "Name": "Andrea Capiluppi",
    "Affiliation": "University of East London"
},
{
    "Name": "Andreas Demuth",
    "Affiliation": "Johannes Kepler University"
},
{
    "Name": "Andreas Goeb",
    "Affiliation": "SAP Research"
},
{
    "Name": "Andreas Metzger",
    "Affiliation": "University of Duisburg-Essen"
},
{
    "Name": "Andreas Zeller",
    "Affiliation": "Saarland University"
},
{
    "Name": "Andrew Bernat",
    "Affiliation": "University of Wisconsin at Madison"
},
{
    "Name": "Andrew Browne",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Andrew Craik",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Andrew Meneely",
    "Affiliation": "North Carolina State University"
},
{
    "Name": "Andy Podgurski",
    "Affiliation": "Case Western Reserve University"
},
{
    "Name": "Angelo Susi",
    "Affiliation": "FBK - IRST CIT"
},
{
    "Name": "Anita Sarma",
    "Affiliation": "University of Nebraska, Lincoln"
},
{
    "Name": "Anna Hannemann"
},
{
    "Name": "Anne Meade",
    "Affiliation": "University of Limerick"
},
{
    "Name": "Anthony Cleve",
    "Affiliation": "University of Namur"
},
{
    "Name": "AntÃ³nia Lopes",
    "Affiliation": "University of Lisbon"
},
{
    "Name": "Antonio Filieri",
    "Affiliation": "Politecnico di Milano"
},
{
    "Name": "Antonio Messina",
    "Affiliation": "University of Rome (Tor Vergata)"
},
{
    "Name": "Antony Tang"
},
{
    "Name": "Arie Gurfinkel",
    "Affiliation": "SEI/CMU"
},
{
    "Name": "Armando Solar-Lezama",
    "Affiliation": "MIT"
},
{
    "Name": "Arun Ramamurthi",
    "Affiliation": "Microsoft India R&D Pvt. Ltd."
},
{
    "Name": "Audris Mockus",
    "Affiliation": "Avaya Labs Research"
},
{
    "Name": "Awais Rashid",
    "Affiliation": "Lancaster University"
},
{
    "Name": "Ayse Basar Bener",
    "Affiliation": "Ryerson University"
},
{
    "Name": "Ayse Tosun Misirli",
    "Affiliation": "Bogazici University"
},
{
    "Name": "Barbara Ryder",
    "Affiliation": "Virginia Tech"
},
{
    "Name": "Barton Miller",
    "Affiliation": "University of Wisconsin at Madison"
},
{
    "Name": "Bashar Nuseibeh",
    "Affiliation": "Lero \u0013 University of Limerick"
},
{
    "Name": "Benjamin Brandl",
    "Affiliation": "University of Passau"
},
{
    "Name": "Bernard Wong",
    "Affiliation": "University of Technology Sydney"
},
{
    "Name": "Bernhard Rumpe",
    "Affiliation": "RWTH Aachen University"
},
{
    "Name": "Bilge Soran",
    "Affiliation": "University of Washington"
},
{
    "Name": "Bo JÃ¸rgensen",
    "Affiliation": "University of Southern Denmark"
},
{
    "Name": "Brady Garvin",
    "Affiliation": "University of Nebraska-Lincoln"
},
{
    "Name": "Bram Adams",
    "Affiliation": "Queen's University"
},
{
    "Name": "Brendan Murphy",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Brian Hackett",
    "Affiliation": "Stanford University"
},
{
    "Name": "Bruno Cafeo",
    "Affiliation": "University of Sao Paulo"
},
{
    "Name": "Carlo Ghezzi",
    "Affiliation": "Politecnico di Milano"
},
{
    "Name": "Carlos Solis",
    "Affiliation": "Lero - University of Limerick"
},
{
    "Name": "CÃ©dric Jeanneret",
    "Affiliation": "Department of Informatics, University of Zurich"
},
{
    "Name": "Chengkai Li",
    "Affiliation": "University of Texas at Arlington"
},
{
    "Name": "Christian Bird",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Christian Hoermann",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Christian Kaestner",
    "Affiliation": "Philipps University Marburg"
},
{
    "Name": "Christian Lengauer",
    "Affiliation": "University of Passau"
},
{
    "Name": "Christian Prause",
    "Affiliation": "Fraunhofer FIT"
},
{
    "Name": "Christoph Csallner",
    "Affiliation": "University of Texas at Arlington"
},
{
    "Name": "Christoph Treude",
    "Affiliation": "University of Victoria"
},
{
    "Name": "Claude Petitpierre",
    "Affiliation": "ÃCOLE POLYTECHNIQUE FÃDÃRALE DE LAUSANNE"
},
{
    "Name": "Claudia Priesterjahn",
    "Affiliation": "University of Paderborn"
},
{
    "Name": "Cleidson de Souza",
    "Affiliation": "IBM Research"
},
{
    "Name": "Corey Jergensen",
    "Affiliation": "University of Nebraska, Lincoln"
},
{
    "Name": "Cristina Cifuentes",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Cristina Marinescu",
    "Affiliation": "Politehnica University of Timisoara"
},
{
    "Name": "Csaba Nagy"
},
{
    "Name": "Daiva Naudziuniene",
    "Affiliation": "University of Cambridge"
},
{
    "Name": "Daniela Costache",
    "Affiliation": "Technische UniversitÃ¤t MÃ¼nchen"
},
{
    "Name": "Daniela Damian"
},
{
    "Name": "Darko Marinov",
    "Affiliation": "University of Illinois"
},
{
    "Name": "Daryl Posnett",
    "Affiliation": "University of California, Davis"
},
{
    "Name": "David Caspar",
    "Affiliation": "Department of Informatics, University of Zurich"
},
{
    "Name": "David Garlan",
    "Affiliation": "Carnegie Mellon University",
    "Bio": "David Garlan is a Professor of Computer Science and Director of Software Engineering Professional Programs in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from Carnegie Mellon in 1987 and worked as a software architect in industry between 1987 and 1990. His interests include software architecture, self-adaptive systems, formal methods, and cyber-physical systems. He is considered to be one of the founders of the field of software architecture, and, in particular, formal representation and analysis of architectural designs. He is a co-author of two books on software architecture. Software Architecture. Perspectives on an Emerging Discipline, and Documenting Software Architecture. Views and Beyond. In 2005 he received a Stevens Award Citation for fundamental contributions to the development and understanding of software architecture as a discipline in software engineering. \n"
},
{
    "Name": "David Notkin",
    "Affiliation": "University of Washington"
},
{
    "Name": "Davide Di Ruscio",
    "Affiliation": "UniversitÃ  dell'Aquila"
},
{
    "Name": "Davide Falessi",
    "Affiliation": "University of Rome (Tor Vergata)"
},
{
    "Name": "Dawei Qi",
    "Affiliation": "National University of Singapore"
},
{
    "Name": "Dennis Pagano"
},
{
    "Name": "Ding Yuan",
    "Affiliation": "University of Illinois at Urbana-Champaign"
},
{
    "Name": "Dino Distefano",
    "Affiliation": "Queen Mary University of London & Monoidics Ltd"
},
{
    "Name": "Diptikalyan Saha",
    "Affiliation": "IBM Research - India"
},
{
    "Name": "Dominik Steenken",
    "Affiliation": "University of Paderborn"
},
{
    "Name": "DongGyun Han",
    "Affiliation": "The Hong Kong University of Science and Technology"
},
{
    "Name": "Dongyun Jin",
    "Affiliation": "University of Illinois"
},
{
    "Name": "Douglas Teoh",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Earl Barr",
    "Affiliation": "University of California, Davis"
},
{
    "Name": "Eda Marchetti",
    "Affiliation": "ISTI-CNR Pisa"
},
{
    "Name": "Ehsan Kouroshfar",
    "Affiliation": "George Mason University"
},
{
    "Name": "Emad Shihab",
    "Affiliation": "Queen's Univeristy"
},
{
    "Name": "Emanuel Giger",
    "Affiliation": "University of Zurich"
},
{
    "Name": "Emily Jacobson",
    "Affiliation": "University of Wisconsin"
},
{
    "Name": "Eric Bodden",
    "Affiliation": "Technische UniversitÃ¤t Darmstadt"
},
{
    "Name": "Ãric Tanter",
    "Affiliation": "Computer Science Department (DCC)"
},
{
    "Name": "Evan Driscoll",
    "Affiliation": "University of Wisconsin"
},
{
    "Name": "Eya Ben Charrada",
    "Affiliation": "Department of Informatics, University of Zurich"
},
{
    "Name": "Fabian Beck",
    "Affiliation": "University of Trier"
},
{
    "Name": "Fabiano Ferrari",
    "Affiliation": "Federal University of Sao Carlos"
},
{
    "Name": "Fabio Calefato"
},
{
    "Name": "Filippo Lanubile"
},
{
    "Name": "Foyzur Rahman",
    "Affiliation": "University of California, Davis"
},
{
    "Name": "Francis Palma",
    "Affiliation": "FBK - IRST CIT"
},
{
    "Name": "Francisco Santana",
    "Affiliation": "Federal University of ParÃ¡ (UFPA)"
},
{
    "Name": "Frank Tip",
    "Affiliation": "IBM Research"
},
{
    "Name": "GÃ¡bor SzabÃ³",
    "Affiliation": "University of Szeged",
    "Bio": "GÃ¡bor SzabÃ³ received his MS and PhD degrees in physics from JATE University, Szeged, Hungary, in 1978 and 1981, respectively. From 1978 to the present he has been working at University of Szeged where he has been a full professor in the Department of Optics and Quantum Electronics since 1994. Since 2010 he has been the rector of the University of Szeged. He has also visited scientists at both Max Planck Institute, GÃ¶ttingen, Germany, and Rice University, Houston, Texas. Dr. SzabÃ³ is a member of the Hungarian Physical Society, he is the chairman of the Hungarian Association for Innovation, and has been a member of Hungarian Academy of Sciences since 2010. His research activities include photoacoustic spectroscopy, ultrafast laser spectroscopy, generation of femtosecond pulses, nonlinear optics, optimum control of quantum systems, medical applications of lasers. \n"
},
{
    "Name": "Gail Kaiser"
},
{
    "Name": "Gail Murphy"
},
{
    "Name": "Georg Kalus",
    "Affiliation": "Technische UniversitÃ¤t MÃ¼nchen"
},
{
    "Name": "George Baah",
    "Affiliation": "Georgia Institute of Technology"
},
{
    "Name": "Giancarlo Succi"
},
{
    "Name": "Giovanni Alluvatti",
    "Affiliation": "University of Sannio"
},
{
    "Name": "Giuseppe De Ruvo",
    "Affiliation": "University of Sannio"
},
{
    "Name": "Giuseppe Santoro"
},
{
    "Name": "Gordon Fraser",
    "Affiliation": "Saarland University"
},
{
    "Name": "Grigore Rosu",
    "Affiliation": "University of Illinois"
},
{
    "Name": "Gustavo Oliva",
    "Affiliation": "University of SÃ£o Paulo (USP)"
},
{
    "Name": "Hans Vliet"
},
{
    "Name": "Harald Gall",
    "Affiliation": "University of Zurich"
},
{
    "Name": "Harry Sneed",
    "Affiliation": "ANECON GmbH"
},
{
    "Name": "Henry Hoffmann",
    "Affiliation": "MIT"
},
{
    "Name": "Hideaki Hata",
    "Affiliation": "Osaka University"
},
{
    "Name": "Hoa Dam",
    "Affiliation": "University of Wollongong"
},
{
    "Name": "Hoang Nguyen",
    "Affiliation": "National University of Singapore"
},
{
    "Name": "Hoh In",
    "Affiliation": "Korea University"
},
{
    "Name": "Hongyu Zhang",
    "Affiliation": "Tsinghua University"
},
{
    "Name": "Ilenia Fronza"
},
{
    "Name": "Inah Omoronyia",
    "Affiliation": "Lero - University of Limerick"
},
{
    "Name": "Indika Meedeniya",
    "Affiliation": "Swinburne University of Technology"
},
{
    "Name": "Irwin Kwan"
},
{
    "Name": "Ivan Beschastnikh",
    "Affiliation": "University of Washington"
},
{
    "Name": "Ivano Malavolta",
    "Affiliation": "Univeristy of L'Aquila"
},
{
    "Name": "J.J. Collins",
    "Affiliation": "University of Limerick"
},
{
    "Name": "Jacob Zimmermann",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Jaechang Nam",
    "Affiliation": "The Hong Kong University of Science and Technology"
},
{
    "Name": "Jafar Al-Kofahi",
    "Affiliation": "Iowa State University"
},
{
    "Name": "Jan Ringert",
    "Affiliation": "RWTH Aachen University"
},
{
    "Name": "Jan Wloka",
    "Affiliation": "IBM Rational Research Lab"
},
{
    "Name": "Javier CÃ¡mara",
    "Affiliation": "University of Coimbra"
},
{
    "Name": "Jeff Foster",
    "Affiliation": "University of Maryland, College Park"
},
{
    "Name": "Jelena Vlasenko"
},
{
    "Name": "Jenny Abrahamson",
    "Affiliation": "University of Washington"
},
{
    "Name": "Jeremias RÃ¶Ãler",
    "Affiliation": "Saarland University"
},
{
    "Name": "JÃ©rÃ´me Vouillon",
    "Affiliation": "CNRS, PPS UMR 7126, Univ Paris Diderot, Sorbonne Paris CitÃ©"
},
{
    "Name": "Jian Lu",
    "Affiliation": "Nanjing University"
},
{
    "Name": "Jim Buckley",
    "Affiliation": "University of Limerick"
},
{
    "Name": "Jochen Wuttke",
    "Affiliation": "University of Washington"
},
{
    "Name": "Joerg Liebig",
    "Affiliation": "University of Passau"
},
{
    "Name": "John Barton",
    "Affiliation": "IBM Research - Almaden"
},
{
    "Name": "John Mylopoulos",
    "Affiliation": "University of Trento"
},
{
    "Name": "Jonathan Bell"
},
{
    "Name": "Joost Noppen",
    "Affiliation": "University of East Anglia"
},
{
    "Name": "Kathryn McKinley",
    "Affiliation": "The University of Texas at Austin & Microsoft Research"
},
{
    "Name": "Katsuhisa Maruyama",
    "Affiliation": "Ritsumeikan University"
},
{
    "Name": "Kivanc Muslu",
    "Affiliation": "University of Washington"
},
{
    "Name": "Klaus Lochmann",
    "Affiliation": "Technische UniversitÃ¤t MÃ¼nchen"
},
{
    "Name": "Kunal Taneja",
    "Affiliation": "North Carolina State University"
},
{
    "Name": "Lakshmi Bairavasundaram",
    "Affiliation": "NetApp Inc."
},
{
    "Name": "Lars Grunske",
    "Affiliation": "University of Kaiserslautern"
},
{
    "Name": "Laurie Williams",
    "Affiliation": "North Carolina State University"
},
{
    "Name": "Leif Singer"
},
{
    "Name": "Leonidas Fegaras",
    "Affiliation": "University of Texas at Arlington"
},
{
    "Name": "Lian Li",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Lionel Briand",
    "Affiliation": "Simula Research Lab"
},
{
    "Name": "Lionel Montrieux",
    "Affiliation": "The Open University"
},
{
    "Name": "Lori Pollock",
    "Affiliation": "University of Delaware"
},
{
    "Name": "Luciano Baresi",
    "Affiliation": "Politecnico di Milano"
},
{
    "Name": "Magnus Madsen",
    "Affiliation": "Aarhus University"
},
{
    "Name": "Mangala Gowri Nanda",
    "Affiliation": "IBM Research - India"
},
{
    "Name": "Manuel Valdiviezo",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Marc Fisher II",
    "Affiliation": "Virginia Tech"
},
{
    "Name": "Marco Autili",
    "Affiliation": "UniversitÃ  dell'Aquila"
},
{
    "Name": "Marco Gerosa",
    "Affiliation": "University of SÃ£o Paulo (USP)"
},
{
    "Name": "Marco Kuhrmann",
    "Affiliation": "Technische UniversitÃ¤t MÃ¼nchen"
},
{
    "Name": "Marco Molfetta",
    "Affiliation": "University of Sannio"
},
{
    "Name": "Marco Mori",
    "Affiliation": "IMT Institute for Advanced Studies Lucca"
},
{
    "Name": "Margaret-Anne Storey",
    "Affiliation": "University of Victoria"
},
{
    "Name": "Maria Jose Escalona",
    "Affiliation": "University of Sevilla"
},
{
    "Name": "Mark Grechanik",
    "Affiliation": "Accenture Technology Labs"
},
{
    "Name": "Mark Harman",
    "Affiliation": "University College London"
},
{
    "Name": "Markus Lumpe",
    "Affiliation": "Swinburne University of Technology"
},
{
    "Name": "Markus von Detten",
    "Affiliation": "University of Paderborn"
},
{
    "Name": "Marsha Chechik",
    "Affiliation": "University of Toronto"
},
{
    "Name": "Martin Glinz",
    "Affiliation": "Department of Informatics, University of Zurich"
},
{
    "Name": "Martin Pinzger",
    "Affiliation": "Delft University of Technology"
},
{
    "Name": "Martin Rinard",
    "Affiliation": "MIT"
},
{
    "Name": "Mary Jean Harrold",
    "Affiliation": "Georgia Institute of Technology"
},
{
    "Name": "Mary Shaw",
    "Affiliation": "Carnegie Mellon University",
    "Bio": "Mary Shaw is the Alan J. Perlis University Professor of Computer Science at Carnegie Mellon University, where she has been a member of the faculty since completing her PhD in 1972. Her research interests lie in the area of software engineering and software systems, particularly software architecture, end user software engineering, cybersociotechnical systems, and software design. She is co-author of Software Architecture. Perspectives on an Emerging Disciplin' and is considered to be one of the founders of the field of software architecture . She has received the ACM SIGSOFT Outstanding Research AWARD, the IEEE Computer Society TCSE's Distinguished Educator Award, CSEE&T's Nancy Mead Award for Excellence in Software Engineering Education, the Stevens Award, and the Warnier Prize. She is a fellow of the Association for Computing Machinery (ACM), the Institute for Electrical and Electronics Engineers (IEEE) and the American Association for the Advancement of Science (AAAS), and she is a member of IFIP WG 2.10 on Software Architecture. She is a past member of the National Research Council's Computer Science and Telecommunications Board and the Defense Advanced Research Project Agencyâs Information Science and Technology Board. \n"
},
{
    "Name": "Masae Yamamuro",
    "Affiliation": "Mitsubishi Research Institute, Inc."
},
{
    "Name": "Massimo Tivoli",
    "Affiliation": "UniversitÃ  dell'Aquial"
},
{
    "Name": "Mathew Hall",
    "Affiliation": "University of Sheffield"
},
{
    "Name": "Mathieu LavallÃ©e",
    "Affiliation": "Ãcole Polytechnique de MontrÃ©al"
},
{
    "Name": "Matko Botincan",
    "Affiliation": "University of Cambridge"
},
{
    "Name": "Matthew Dwyer",
    "Affiliation": "University of Nebraska-Lincoln"
},
{
    "Name": "Matthew Parkinson",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Matthias Riebisch",
    "Affiliation": "Ilmenau University of Technology"
},
{
    "Name": "Matthias Schur",
    "Affiliation": "SAP Research"
},
{
    "Name": "Matthias Tichy",
    "Affiliation": "University of Augsburg"
},
{
    "Name": "Mauro Baluda",
    "Affiliation": "University of Lugano"
},
{
    "Name": "Mazeiar Salehie",
    "Affiliation": "Lero - University of Limerick"
},
{
    "Name": "Mehdi Mirzaaghaei",
    "Affiliation": "University of Lugano"
},
{
    "Name": "Mehrdad Sabetzadeh",
    "Affiliation": "Simula Research Lab"
},
{
    "Name": "Michael Ernst",
    "Affiliation": "University of Washington"
},
{
    "Name": "Michael Lyu",
    "Affiliation": "The Chinese University of Hong Kong"
},
{
    "Name": "Michael Sloan",
    "Affiliation": "University of Washington"
},
{
    "Name": "Michel Wermelinger",
    "Affiliation": "The Open University"
},
{
    "Name": "Mike Dodds",
    "Affiliation": "University of Cambridge"
},
{
    "Name": "Milos Gligoric",
    "Affiliation": "University of Illinois at Urbana-Champaign"
},
{
    "Name": "Milton Inostroza",
    "Affiliation": "University of Chile"
},
{
    "Name": "Minh Ngo",
    "Affiliation": "National University of Singapore"
},
{
    "Name": "Miryung Kim",
    "Affiliation": "The University of Texas at Austin"
},
{
    "Name": "Moonzoo Kim",
    "Affiliation": "Korea Advanced Institute of Science and Technology"
},
{
    "Name": "Motoei Azuma",
    "Affiliation": "Waseda University"
},
{
    "Name": "Motoei AZUMA",
    "Affiliation": "Waseda University"
},
{
    "Name": "Mutsumi Abe"
},
{
    "Name": "Myra Cohen",
    "Affiliation": "University of Nebraska-Lincoln"
},
{
    "Name": "Na Meng",
    "Affiliation": "The University of Texas at Austin"
},
{
    "Name": "Nachiappan Nagappan",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Naeem Esfahani",
    "Affiliation": "George Mason University"
},
{
    "Name": "Nathan Hawes",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Nathan Keynes",
    "Affiliation": "Oracle Labs"
},
{
    "Name": "Nathan Rosenblum",
    "Affiliation": "University of Wisconsin"
},
{
    "Name": "Neil Ernst",
    "Affiliation": "University of Toronto"
},
{
    "Name": "Nicola Sanitate"
},
{
    "Name": "Nicolas Bettenburg",
    "Affiliation": "Queen's University, School of Computing"
},
{
    "Name": "Norbert Seyff"
},
{
    "Name": "Ohad Barzilay",
    "Affiliation": "Tel-Aviv University"
},
{
    "Name": "Omer Tripp"
},
{
    "Name": "Omri Weisman"
},
{
    "Name": "Orit Hazzan",
    "Affiliation": "Technion - Israel Institute of Technology"
},
{
    "Name": "Osamu Mizuno",
    "Affiliation": "Kyoto Institute of Technology"
},
{
    "Name": "Pankaj Dhoolia",
    "Affiliation": "IBM Research - India"
},
{
    "Name": "Paola Inverardi",
    "Affiliation": "UniversitÃ  dell'Aquila"
},
{
    "Name": "Paolo Tonella",
    "Affiliation": "FBK - IRST CIT"
},
{
    "Name": "Patrick Wagstrom",
    "Affiliation": "IBM TJ Watson Research Center"
},
{
    "Name": "Patrizio Pelliccione",
    "Affiliation": "UniversitÃ  dell'Aquila"
},
{
    "Name": "Paul Clements",
    "Affiliation": "Carnegie Mellon University"
},
{
    "Name": "Pete Rotella",
    "Affiliation": "Cisco Systems, Inc."
},
{
    "Name": "Peter Nacken"
},
{
    "Name": "Pierre Robillard",
    "Affiliation": "Ãcole Polytechnique de MontrÃ©al"
},
{
    "Name": "Prem Devanbu"
},
{
    "Name": "Premkumar Devanbu",
    "Affiliation": "University of California, Davis"
},
{
    "Name": "Qing Wang",
    "Affiliation": "Institute of Software, Chinese Academy of Sciences"
},
{
    "Name": "Qingzhou Luo",
    "Affiliation": "University of Illinois"
},
{
    "Name": "Qirun Zhang",
    "Affiliation": "The Chinese University of Hong Kong"
},
{
    "Name": "Radu Calinescu",
    "Affiliation": "Aston University"
},
{
    "Name": "Radu Grigore",
    "Affiliation": "Queen Mary University of London"
},
{
    "Name": "Raian Ali",
    "Affiliation": "Lero - University of Limerick"
},
{
    "Name": "Ralph Johnson",
    "Affiliation": "University of Illinois at Urbana-Champaign"
},
{
    "Name": "Rayid Ghani",
    "Affiliation": "Accenture Technology Labs"
},
{
    "Name": "Reid Holmes",
    "Affiliation": "University of Waterloo"
},
{
    "Name": "Rishabh Singh",
    "Affiliation": "MIT"
},
{
    "Name": "Robert Brcina",
    "Affiliation": "Ilmenau University of Technology"
},
{
    "Name": "Robert Nilsson"
},
{
    "Name": "Roberto Di Cosmo",
    "Affiliation": "Univ Paris Diderot, Sorbonne Paris CitÃ©, PPS, UMR 7126 CNRS"
},
{
    "Name": "Roberto Lopez-Herrejon",
    "Affiliation": "Johannes Kepler University"
},
{
    "Name": "Roberto Lublinerman",
    "Affiliation": "Pennsylvania State University"
},
{
    "Name": "RogÃ©rio de Lemos",
    "Affiliation": "University of Kent"
},
{
    "Name": "Romain Robbes",
    "Affiliation": "University of Chile"
},
{
    "Name": "Rongxin Wu",
    "Affiliation": "Tsinghua University"
},
{
    "Name": "Rudolf Ferenc"
},
{
    "Name": "Rudolf Majnar",
    "Affiliation": "SoRing Kft."
},
{
    "Name": "Ruzanna Chitchyan",
    "Affiliation": "Lancaster University"
},
{
    "Name": "Salman Mirghasemi",
    "Affiliation": "ÃCOLE POLYTECHNIQUE FÃDÃRALE DE LAUSANNE"
},
{
    "Name": "Sam Malek",
    "Affiliation": "George Mason University"
},
{
    "Name": "Samuel Fricker"
},
{
    "Name": "Samuel Madden",
    "Affiliation": "MIT CSAIL"
},
{
    "Name": "Sandro Badame",
    "Affiliation": "University of Illinois at Urbana-Champaign"
},
{
    "Name": "Sara Navidpour",
    "Affiliation": "Pennsylvania State University"
},
{
    "Name": "Sasa Misailovic",
    "Affiliation": "MIT"
},
{
    "Name": "Satish Chandra",
    "Affiliation": "IBM T. J. Watson Research Center"
},
{
    "Name": "Sebastian Uchitel",
    "Affiliation": "University of Buenos Aires"
},
{
    "Name": "Shahar Maoz",
    "Affiliation": "RWTH Aachen University"
},
{
    "Name": "Shankar Pasupathy",
    "Affiliation": "NetApp Inc."
},
{
    "Name": "Shin Yoo"
},
{
    "Name": "Shing-Chi Cheung",
    "Affiliation": "The Hong Kong University of Science and Technology"
},
{
    "Name": "Shin'ichi Shiraishi"
},
{
    "Name": "Shiva Nejati",
    "Affiliation": "Simula Research Lab"
},
{
    "Name": "Shoham Ben-David",
    "Affiliation": "University of Toronto"
},
{
    "Name": "Sigurd Schneider",
    "Affiliation": "Saarland University"
},
{
    "Name": "Simon Jensen",
    "Affiliation": "Aarhus University"
},
{
    "Name": "Srikant Y. N",
    "Affiliation": "Indian Institute of Science"
},
{
    "Name": "Stefan Wagner",
    "Affiliation": "University of Stuttgart"
},
{
    "Name": "Steffen Lehnert",
    "Affiliation": "Ilmenau University of Technology"
},
{
    "Name": "Stelios Sidiroglou-Douskos",
    "Affiliation": "MIT"
},
{
    "Name": "Stephan Bode",
    "Affiliation": "Ilmenau University of Technology"
},
{
    "Name": "Stephan Diehl",
    "Affiliation": "University of Trier"
},
{
    "Name": "Subhajit Roy",
    "Affiliation": "Indian Institute of Technology"
},
{
    "Name": "Sumit Gulwani",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Sunghun Kim",
    "Affiliation": "The Hong Kong University of Science and Technology"
},
{
    "Name": "Sunita Chulani",
    "Affiliation": "Cisco Systems"
},
{
    "Name": "Sven Apel",
    "Affiliation": "University of Passau"
},
{
    "Name": "Swapneel Sheth"
},
{
    "Name": "Swarat Chaudhuri",
    "Affiliation": "Rice University"
},
{
    "Name": "Taek Lee",
    "Affiliation": "Korea University"
},
{
    "Name": "Takayuki Omori",
    "Affiliation": "Ritsumeikan University"
},
{
    "Name": "Tao Sun",
    "Affiliation": "National University of Singapore"
},
{
    "Name": "Tao Xie",
    "Affiliation": "North Carolina State University"
},
{
    "Name": "Thomas Fritz"
},
{
    "Name": "Thomas Reps",
    "Affiliation": "University of Wisconsin"
},
{
	"Id" : 1819604,
    "Name": "Thomas Zimmermann",
    "Affiliation": "Microsoft Research"
},
{
    "Name": "Tibor Bakota"
},
{
    "Name": "Tien Nguyen",
    "Affiliation": "Iowa State University"
},
{
    "Name": "Tohru Kikuno",
    "Affiliation": "Osaka University"
},
{
    "Name": "Tom Arbuckle",
    "Affiliation": "University of Limerick"
},
{
    "Name": "Ton Gerrits"
},
{
    "Name": "Toshihiro Komiyama",
    "Affiliation": "NEC Corporation"
},
{
    "Name": "Tuli Nivas"
},
{
    "Name": "Tung Nguyen",
    "Affiliation": "Iowa State University"
},
{
    "Name": "V. Krishna Nandivada",
    "Affiliation": "IBM Research - India"
},
{
    "Name": "Valerio Panzica La Manna",
    "Affiliation": "Politecnico di Milano"
},
{
    "Name": "Vibha Sinha",
    "Affiliation": "IBM Research - India"
},
{
    "Name": "Vilas Jagannath",
    "Affiliation": "University of Illinois"
},
{
    "Name": "Vincenzo De Florio",
    "Affiliation": "Universiteit Antwerpen"
},
{
    "Name": "Vittorio Cortellessa",
    "Affiliation": "UniversitÃ  dell'Aquila"
},
{
    "Name": "Walid Maalej",
    "Affiliation": "Technische UniverstitÃ¤t MÃ¼nchen"
},
{
    "Name": "Wen Zhang",
    "Affiliation": "Institute of Software, Chinese Academy of Sciences"
},
{
    "Name": "Wilhelm SchÃ¤fer",
    "Affiliation": "University of Paderborn",
    "Bio": "Dr. Wilhelm SchÃ¤fer, born August 16th 1954, got his PhD degree 1986 in the area of software engineering from the University of OsnabrÃ¼ck, Germany. 1986 -1987 he spent as a Visiting Assistant Professor at McGill University Montreal, Canada. From 1986 to 1990 he was head of research and development at STZ company for Software Technology ltd., Dortmund. From 1991 to 1994 he was Associate Professor, Department of Computer Science, University of Dortmund. Since 1994 he is full professor and chair, head of Software Engineering Group, Department of Computer Science, University of Paderborn. Prof. SchÃ¤fer is also the chair of the International Graduate School of the University of Paderborn and deputy chair of the collaborative research centre (CRC 614 Self-Optimization in Mechanical Engineering). He was and is member of many national and international program committees in software engineering. He was a member of the IEEE Transactions on Software Engineering Editorial Board, PC-Chair of the 5th European Software Engineering Conference (ESEC), Barcelona in 1995, PC Co-Chair of the 23rd International Conference on Software Engineering in Toronto in 2001 and General Chair of the 30th International Conference on Software Engineering in Leipzig in 2008. Since 2010 he is Co-Director of the newly founded Fraunhofer group on Mechatronic System Design in Paderborn and also serves as Vice-President Research of the University of Paderborn since 2003. His main research interests are in Model-based Development of Embedded and Mechatronic Systems, Re-Engineering and Software Process Modeling as well as Version- and Configuration Management. \n"
},
{
    "Name": "William Langdon",
    "Affiliation": "University College London"
},
{
    "Name": "Wujie Zheng",
    "Affiliation": "The Chinese University of Hong Kong"
},
{
    "Name": "Xiaoxing Ma",
    "Affiliation": "Nanjing University"
},
{
    "Name": "Yan Wang"
},
{
    "Name": "Yana Mileva",
    "Affiliation": "Saarland University"
},
{
    "Name": "Yasutaka Kamei",
    "Affiliation": "Queen's University"
},
{
    "Name": "Ye Yang",
    "Affiliation": "Institute of Software, Chinese Academy of Sciences"
},
{
    "Name": "Yijun Yu",
    "Affiliation": "The Open University"
},
{
    "Name": "Yoonkyu Jang"
},
{
    "Name": "Yuanyuan Zhou",
    "Affiliation": "University of California, San Diego"
},
{
    "Name": "Yue Jia",
    "Affiliation": "University College London"
},
{
    "Name": "Yukio Tanitsu",
    "Affiliation": "IBM Japan, Ltd."
},
{
    "Name": "Yunho Kim",
    "Affiliation": "Korea Advanced Institute of Science and Technology"
},
{
    "Name": "Yuriy Brun",
    "Affiliation": "University of Washington"
},
{
    "Name": "Zoya Durdik",
    "Affiliation": "Research Center for Information Technology (FZI)"
},
{
    "Name": "Zuoning Yin",
    "Affiliation": "University of Illinois at Urbana-Champaign"
}
	]
});